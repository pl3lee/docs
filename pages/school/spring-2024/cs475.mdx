import { Definition, DefinitionName, DefinitionContent, Corollary, CorollaryName, CorollaryContent, Example, ExampleName, ExampleContent, Problem, ProblemName, ProblemContent, Proposition, PropositionName, PropositionContent, Theorem, TheoremName, TheoremContent, Lemma, LemmaName, LemmaContent, Proof, ProofName, ProofContent } from "/components/math";
import { Steps, Callout } from 'nextra/components'

# CS 475: Computational Linear Algebra
## Introduction
### What is Numerical Linear Algebra?
Briefly, it is developing **efficient** algorithms to **accurately** solve linear algebra problems on a computer (with finite precision arithmetic). We will focus on the following topics:
- solving linear systems of equations.
- solving linear least squares problems.
- solving eigenvalue problems.
- performing singular value decompositions.

### Types of Algorithms
There are two types of algorithms:
- **Direct**: These algorithms compute the final solution with a finite sequence of arithmetic operations. For example, Gaussian elimination (LU factorization)
- **Iterative**: These algorithms start with an initial guess, and iteratively apply the same operation to get closer to the solution, until it is good enough. For example, Jacobi, Gauss-Seidel, Conjugate Gradient, and PageRank.

### Matrix Structure
There are two types of matrices:
- **Dense**: Most of the entries are non-zero. These are stored in $N \times N$ arrays, which we can manipulate normally.
- **Sparse**: Most entries are zero. Non-zero locations may exhibit patterns.

It is possible to exploit sparsity patterns/structure to save memory and/or flops. However, just because a matrix is sparse, it does not mean that it will be faster to solve. It depends on the sparsity pattern and the algorithm used. For example, even if we input a sparse matrix into a Gaussian elimination algorithm, it will still take the same amount of time.

### Factorization
A common theme is that we can express a given matrix as a product of other matrices. For example,
$$
A = BCD
$$
Some common factorizations are:
- LU factorization: $A = LU$
- QR factorization: $A = QR$
- Cholesky factorization: $A = LL^T$
- Singular Value Decomposition: $A = U \Sigma V^T$
- etc.

These factorizations have properties that we can exploit in algorithm design.

### Orthogonality
Notions of orthogonality will arise repeatedly in designing algorithms. Recall the following definitions for orthogonality.
<Definition withName={true}>
    <DefinitionName>Orthogonal Vectors</DefinitionName>
    <DefinitionContent>
        Two vectors $x$ and $y$ are orthogonal if $x^T y = 0$.
    </DefinitionContent>
</Definition>
<Definition withName={true}>
    <DefinitionName>Orthogonal Matrices</DefinitionName>
    <DefinitionContent>
        A real square matrix $Q$ is orthogonal if $Q^T Q = I$, or $Q^T = Q^{-1}$.
    </DefinitionContent>
</Definition>

### Linear Systems
This is the problem of solving $Ax = b$ for $x$ where $A \in \mathbb R^{n \times n}, x \in \mathbb R^n, b \in \mathbb R^n$. Doing it efficiently and accurately on large problems (in floating point arithmetic) can take substantial effort to get right.

For example, if we have the system $Ax = b$, then we could rearrange it to get $x = A^{-1}b$. However, computing the inverse of $A$ is potentially expensive and numerically unstable.

### Least Squares Problem
This is used for solving more general problems which may have
- Too many equations/constraints (over-determined).

    For example, $A \in \mathbb R^{30 \times 2}, b \in \mathbb R^{30}$, i.e. the matrix $A$ is tall and skinny.

- Too few equations/constraints (under-determined).

    For example, $A \in \mathbb R^{2 \times 30}, b \in \mathbb R^2$, i.e. the matrix $A$ is short and fat.

We aim to find an answer that is as good as possible. This is often done on data-fitting or regression problems.

If we want to solve $Ax = b$, rearranging gives $Ax - b = 0$. Therefore, we can minimize the norm $||Ax - b||_2$, where the two norm is defined as $||x||_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$. Notice that this is equivalent to minimizing $||Ax - b||_2^2$.

In regression problems, we may have $n$ datapoints $(x_1,y_1),\ldots,(x_n,y_n)$, and we want to find a line that is closest to these points. In the best case, a line is able to pass through all the points. In this case, we have the system of equations
$$
\begin{align*}
    y_1 &= ax_1 + c \\
    y_2 &= ax_2 + c \\
    &\vdots \\
    y_n &= ax_n + c
\end{align*}
$$
which can be written as $Ax = b$ where
$$
\underbrace{\begin{pmatrix}
    x_1 & 1 \\
    x_2 & 1 \\
    \vdots & \vdots \\
    x_n & 1
\end{pmatrix}}_{A} \underbrace{\begin{pmatrix}
    a \\
    c
\end{pmatrix}}_{x} = \underbrace{\begin{pmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
\end{pmatrix}}_{b}
$$
If we minimize $||Ax - b||_2^2$, then in the best case, we will get a line that passes through all the points. Even if we are not in the best case, we still get a line that is as close as possible to the points.

### Eigenvalue Problems
Find the eigenvalues $\lambda$ and eigenvectors $v$ of a $n \times n$ matrix $A$ such that
$$
Av = \lambda v
$$
What does this mean geometrically? Recall that matrices are linear transformations and it maps vectors by stretching, rotating, or reflecting them. Then eigenvectors are vectors such that it only gets stretched by a scalar factor, and the direction remains the same.

The eigenvalue problem is related to a factorization of $A$ into
$$
A = Q\Lambda Q^{-1}
$$
where $\Lambda$ is a diagonal matrix, and columns of $Q$ are eigenvectors of $A$.

We also have the definition of invariant:
<Definition>
    <DefinitionName>Invariant</DefinitionName>
    <DefinitionContent>
        Something that does not change under a transformation. So in the eigenvalue problem, the span of an eigenvector is invariant under $A$.
    </DefinitionContent>
</Definition>

### Singular Value Decomposition (SVD)
Given a general matrix $A$, how do we associate it with a square matrix? We can simply take $A^TA$ or $AA^T$, and they are square matrices. The SVD is a generalization of the eigenvalue problem that applies to all matrices. We solve for a factorization of a general $m \times n$ matrix $A$ such that
$$
A = U \Sigma V^T
$$
where
- $U, V$ are orthogonal matrices
- $\Sigma$ is diagonal with non-negative entries (called the singular values of $A$), which are found by taking the square root of the eigenvalues of $A^TA$ or $AA^T$.
 
## Background and LU Factorization
### Background
Here are some basic definitions.
<Definition>
    <DefinitionName>Vector Space</DefinitionName>
    <DefinitionContent>
        A vector space is a collection/set of vectors, together with addition and scalar multiplication.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Basis</DefinitionName>
    <DefinitionContent>
        A set of vectors $B$, is a basis for a vector space $V$ if
        - $B$ is linearly independent.
        - Every element of $V$ is a linear combination of elements of $B$.
    
    We can say that $B$ spans $V$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Range</DefinitionName>
    <DefinitionContent>
        The range of a given matrix $A$ is defined by
        $$
        \mathrm{range}(A) = \{Ax : x \in \mathbb R^n\}
        $$
        which is the space of vectors that can be generated by left-multiplying some vector $x$ with the matrix $A$.
    </DefinitionContent>
</Definition>
Note that we can view matrix-vector multiplication as taking a linear combination of $A$'s columns, where $x_i$'s gives the coefficients for each column:
$$
\begin{align*}
    \begin{pmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_n
    \end{pmatrix} &= \begin{pmatrix}
    a_1 & a_2 & \cdots & a_n
    \end{pmatrix} \begin{pmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix} \\
    &= x_1a_1 + x_2a_2 + \cdots + x_na_n
\end{align*}
$$
where each $a_1, a_2, \ldots, a_n$ are the columns of $A$. Thus, all vectors in the range of $A$ can be expressed as linear combinations of columns of $A$. Therefore, range is also called teh **column space**.
<Definition>
    <DefinitionName>Row Space</DefinitionName>
    <DefinitionContent>
        The row space is analogous to column space, which is the space of vectors that can be written as linear combinations of the rows of $A$. In terms of matrix multiplication, this is the space of vectors that can be generated by right-multiplying some vector $x$ with the matrix $A$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Nullspace</DefinitionName>
    <DefinitionContent>
        The nullspace or kernet of a matrix $A$ is defined by
        $$
        \mathrm{null}(A) = \{x : Ax = 0\}
        $$
        intuitively, it is the part of the vector space that gets mapped to the zero vector by $A$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Dimension of a Vector Space</DefinitionName>
    <DefinitionContent>
        If a vector space $V$ has a basis of $n$ elements, we say that the dimension of $V$, denoted by $\dim(V)$, is $n$. That is, $\dim(V)$ is the number of linearly independent vectors needed to span $V$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Rank</DefinitionName>
    <DefinitionContent>
        The column rank is the dimension of the column space of a matrix $A$. The row rank is the dimension of the row space of a matrix $A$. 

        It can be proven that column rank $=$ row rank. This is because $A$ can be factorized into $A = PEQ$, where $E = \begin{pmatrix} I & 0 \\ 0 & 0 \end{pmatrix}$, and $P, Q$ are invertible matrices. That is, $E$ is a diagonal matrix starting with $1$'s and ending with $0$'s. Then, the number of linearly independent columns of $A$ is the same as the number of linearly independent rows of $A$.

        Since both column rank and row rank are equal, we can simply refer to it as the rank of $A$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Nullity</DefinitionName>
    <DefinitionContent>
        The dimension of the nullspace is the nullity of $A$. 
    </DefinitionContent>
</Definition>
<Theorem withName={true}>
    <TheoremName>Rank-Nullity Theorem</TheoremName>
    <TheoremContent>
        We have
        $$
        \mathrm{rank}(A) + \mathrm{nullity}(A) = n
        $$
        where $n$ is the number of columns of $A$.
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Let $r$ be the rank and $k$ be the nullity. Suppose that $A: V \to W$ where $V$ is $n$-dimensional and $W$ is $t$ dimensional. Suppose that the nullspace has basis $\{v_1,\ldots,v_k\}$, and suppose that $V$ has basis
        $$
        \{v_1,\ldots,v_k,v_{k+1},\ldots,v_n\}
        $$
        Then, applying $A$ to the basis vectors, we get the image of $A$:
        $$
        \{A(v_1),\ldots,A(v_k),A(v_{k+1}),\ldots,A(v_n)\} = \{Av_{k+1},\ldots,Av_n\}
        $$
        since $Av_i = 0$ for $i = 1,\ldots,k$. This set is linearly independent, and so the rank of $A$ is $n - k = r$. Therefore, we have
        $$
        r + k = n
        $$
    </ProofContent>
</Proof>
Note that a matrix is of full rank if the rank is equal to the minimum of the number of rows and columns. We have the following theorem:
<Theorem>
    <TheoremContent>
        A full rank matrix defines a one-to-one map, that is $Av_1 \neq Av_2$ for any $v_1 \neq v_2$.
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Suppose that $v_1 \neq v_2$ and for contradiction we have $Av_1 = Av_2$. Then $A(v_1 - v_2) = 0$, which implies that $v_1 - v_2 \in \mathrm{null}(A)$. But, since $A$ is full rank, $\mathrm{null}(A) = \{0\}$, and so $v_1 - v_2 = 0$, which is a contradiction.
    </ProofContent>
</Proof>
<Definition>
    <DefinitionName>Invertible</DefinitionName>
    <DefinitionContent>
        A square matrix of full rank is called invertible or nonsingular. The inverse of $A$ is denoted by $A^{-1}$, and is a unique matrix that satisfies
        $$
        AA^{-1} = A^{-1}A = I
        $$
        where $I$ is the identity matrix.
    </DefinitionContent>
</Definition>
We have the following useful properties of invertible matrices:
<Theorem>
    <TheoremContent>
        For a real square matrix $A \in \mathbb R^{n \times n}$, the following are equivalent:
        - $A$ is invertible.
        - $A$ is full rank, so $\mathrm{rank}(A) = n$.
        - $\mathrm{range}(A) = \mathbb R^n$.
        - $\mathrm{null}(A) = \{0\}$.
        - $A$ has no zero eigenvalues.
        - $A$ has no zero singular values.
        - $\det(A) \neq 0$.
    </TheoremContent>
</Theorem>
And we have some identies:
<Theorem withName={true}>
    <TheoremName>Invertible Matrix Identities</TheoremName>
    <TheoremContent>
        For invertible matrices $A$ and $B$:
        - $(AB)^{-1} = B^{-1}A^{-1}$
        - $(A^T)^{-1} = (A^{-1})^T = A^{-T}$
        - $B^{-1} = A^{-1} - B^{-1}(B - A)A^{-1}$
    </TheoremContent>
</Theorem>
Given a matrix $A$, and say you have already found its inverse. If you want to change $A$ slightly by adding $uv^T$, how does $A^{-1}$ change? Turns out we have a formula for this:
<Definition>
    <DefinitionName>Sherman-Morrison</DefinitionName>
    <DefinitionContent>
        If $1 + v^TA^{-1}u \neq 0$, then
        $$
        (A + uv^T)^{-1} = A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u}
        $$
        for $A \in \mathbb R^{n \times n}$, $u,v \in \mathbb R^n$.
    </DefinitionContent>
</Definition>
Notice the denominator $1 + v^TA^{-1}u$. How is matrix division defined? Turns out that $v^T A^{-1} u \in \mathbb R$ is a scalar, and so we can divide by it. This formula allows us to update $A^{-1}$ without starting from scratch. This is called a rank-one update, since the matrix $uv^T$ has rank one. A generalization of this formula is the Sherman-Morrison-Woodbury formula. 
<Definition>
    <DefinitionName>Sherman-Morrison-Woodbury</DefinitionName>
    <DefinitionContent>
        If $A \in \mathbb R^{n \times n}$ is invertible, $U, V \in \mathbb R^{n \times k}$, then
        $$
        (A + UV^T)^{-1} = A^{-1} - A^{-1} - A^{-1}U(I + V^TA^{-1}U)^{-1}V^TA^{-1}
        $$
        That is, a rank $k$ modification to matrix $A$ yields a rank $k$ correction to $A^{-1}$.
    </DefinitionContent>
</Definition>

### LU Factorization
A lot of practical problems rely on solving systems of linear equations of the form $Ax = b$. We will start by looking at direct methods for $Ax = b$. We will strive to avoid constructing $A^{-1}$ itself, since this can be
- less efficient (in terms of operation counts).
- less accurate (incurs more round-off errors).
- more costly in terms of storage (depending on matrix sparsity)

We also define forward and backward errors. Suppose that we compute $y = f(x)$. We might get $\hat y = f(x)$. The forward error is $||\hat y - y||$. Then, we might also have that $\hat y$ is the exact solution for $\hat y = f(x + \Delta x)$. The backward error is $||\Delta x||$. 

Now, we can interpret Gaussian elimination as follows:
1. Factor matrix $A$ into $A = LU$, where $L$ and $U$ are triangular.
2. Solve $Ly = b$ for intermediate vector $y$.
3. Solve $Ux = y$ for $x$.

This works because solving $Ax = b$ is equivalent to solving $LUx = b$, which is equivalent to solving $Ly = b$ by letting $y = Ux$.

We have the following pseudocode for LU factorization
- $k = 1,\ldots, n$ **do** (iterating over all rows)
   - **for** $i = k+1,\ldots, n$ **do** (iterate over each row $i$ beneath row $k$)
      - mult $= \frac{a_{ik}}{a_{kk}}$ (determine row $i$'s multiplicative factor)
      - $a_{ik} = $ mult (store the multiplicative factor in the matrix)
      - **for** $j = k+1,\ldots, n$ **do** (iterate over each column for current row)
         - $a_{ij} = a_{ij} - $ mult $\times a_{kj}$ (update the matrix)
      - **end for**
   - **end for**
- **end for**

So, suppose $A$ row reduces to $A^{(4)}$ by performing 3 row operations. We can store these row operations in identity matrices $M^{(1)}, M^{(2)}, M^{(3)}$ such that
$$
A^{(4)} = M^{(3)}M^{(2)}M^{(1)}A
$$
Now, if we set $A^{(4)} = U$, where $U$ is an upper triangular matrix, then we have
$$
\begin{align*}
    A &=  (M^{(1)})^{-1}(M^{(2)})^{-1}(M^{(3)})^{-1}U \\
      &= L U
\end{align*}
$$
So, we have $L = (M^{(1)})^{-1}(M^{(2)})^{-1}(M^{(3)})^{-1}$. We define $L_j$ as the inverse of the matrix $M^{(j)}$.

$L_j$ can be obtained from $M^{(j)}$ be swapping the signs of the off-diagonal elements. For example, consider the matrix $M^{(3)} = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & -\frac{1}{2} & 1
\end{bmatrix}$. We can obtain $L_3$ by swapping the sign of the off-diagonal element, so $L_3 = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & \frac{1}{2} & 1
\end{bmatrix}$.

Moreover, $L$ can be obtained from all $L_j$'s by placing all of the off-diagonal elements in the matrices into the corresponding position in $L$. So, consider matrices
$$
\begin{align*}
    M^{(1)} &= \begin{bmatrix}
        1 & 0 & 0 \\
        0.3 & 1 & 0 \\
        0 & 0 & 1
        \end{bmatrix} \\
    M^{(2)} &= \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        -0.5 & 0 & 1
        \end{bmatrix} \\
    M^{(3)} &= \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 25 & 1
        \end{bmatrix}
\end{align*}
$$
Then, we have
$$
\begin{align*}
    L &= L_1L_2L_3 \\
    &= (M^{(1)})^{-1}(M^{(2)})^{-1}(M^{(3)})^{-1} \\
    &= \begin{bmatrix}
        1 & 0 & 0 \\
        -0.3 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix} \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0.5 & 0 & 1
    \end{bmatrix} \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & -25 & 1
    \end{bmatrix} \\
    &= \begin{bmatrix}
        1 & 0 & 0 \\
        -0.3 & 1 & 0 \\
        0.5 & -25 & 1
    \end{bmatrix}
\end{align*}
$$

Let's try to solve $\begin{bmatrix}
    1 & 1 & 1 \\
    1 & -2 & 2 \\
    1 & 2 & -1
\end{bmatrix} \begin{bmatrix}
    x_0 \\ x_1 \\ x_2
\end{bmatrix} = \begin{bmatrix}
    0 \\ 4 \\ 2
\end{bmatrix}$ for the vector $x$.

First, we will factor $A$ into $LU$.
1. We start with the intial matrix $A$ and the $L$ the identity matrix
    $$
    A = \begin{bmatrix}
        1 & 1 & 1 \\
        1 & -2 & 2 \\
        1 & 2 & -1
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
    $$
2. We row reduce by $R_2 \to R_2 - R_1$ to obtain
    $$
    A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        1 & 2 & -1
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
    $$
3. We row reduce by $R_3 \to R_3 - R_1$ to obtain
    $$
    A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        0 & 1 & -2
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        1 & 0 & 1
    \end{bmatrix}
    $$
4. We row reduce by $R_3 \to R_3 - \left(-\frac{1}{3}\right)R_2$ to obtain
    $$
    U = A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        0 & 0 & -\frac{5}{3}
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        1 & -\frac{1}{3} & 1
    \end{bmatrix}
    $$

Then, given $b = \begin{bmatrix}
    0 \\ 4 \\ 2
\end{bmatrix}$, we can find $x$ such that $Ax = b$ using forward/backward solve. We first do forward solve $Ly = b$ for $y$.
$$
\begin{align*}
    \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        1 & -\frac{1}{3} & 1
    \end{bmatrix} \begin{bmatrix}
        y_1 \\ y_2 \\ y_3
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 4 \\ 2
    \end{bmatrix} \\
    \begin{bmatrix}
        y_1 \\ y_2 \\ y_3
    \end{bmatrix} = \begin{bmatrix}
    0 \\ 4 \\ \frac{10}{3}
    \end{bmatrix}
\end{align*}
$$
Why is it called forward solve? Because we solve $y_1$ first, then $y_2$, and finally $y_3$. Now, we do backward solve $Ux = y$ for $x$.
$$
\begin{align*}
    \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        0 & 0 & -\frac{5}{3}
    \end{bmatrix} \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 4 \\ \frac{10}{3}
    \end{bmatrix} \\
    \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
    \end{bmatrix} = \begin{bmatrix}
        4 \\ -2 \\ -2
    \end{bmatrix}
\end{align*}
$$

So, the solution to the system of equations is $x = \begin{bmatrix}
    4 \\ -2 \\ -2
\end{bmatrix}$.

Now, why is two matrix solves better than one? It is because we can often reuse the factorization to solve for different $b$'s. So, we have the following pseudo code:
```python
# Forward solve

# loops for i = 1,..., n
for i in range(1, n + 1):
    y_i = b_i
    # loops for j = 1,..., i - 1
    for j in range(1, i):
        y_i = y_i - l_ij * y_j
```

```python
# Backward solve

# loops for i = n,..., 1
for i in range(n, 0, -1):
    x_i = y_i
    # loops for j = i + 1,..., n
    for j in range(i + 1, n + 1):
        x_i = x_i - u_ij * x_j
    x_i = x_i / u_ii
```
We want to know the asymptotic cost to solve a system of size $n$. We will measure the cost in total FLOPs: floating point operations. This is approximated as the number of additions and multiplications. The cost of factorization is
$$
\sum^n_{k=1} \left(\sum^n_{i = k + 1}\left(1 + \sum^n_{j=k+1} 2\right)\right) = \frac{2n^3}{3} + O(n^2)
$$
where $\sum^n_{i=k+1}$ is the middle loop, and $\sum^n_{j=k+1}$ is the innermost loop. In the inner loop, there are 2 operations, and in the middle loop, there is 1 operation.

So LU factorization costs approximately $O(n^3)$ FLOPs. What about forward and backward solves? Counting gives $n^2 + O(n)$ FLOPs for each triangular solve. Thus the total is $O(n^3)$. For large $n$, the cost of factorization is dominant. Then, given a factorization, solving an additional RHS is cheap, only $O(n^2)$.

### Finding the Inverse
Given a matrix $A$, how might we construct $A^{-1}$, if we really wanted to? We define $e_i$ to be the $i$th column in the identity matrix. Suppose that we let
$$
A^{-1} = \begin{bmatrix}
    v_1 & | & v_2 & | & \cdots & | & v_n
\end{bmatrix}
$$
We can then factor $A = LU$. Then, notice that we are finding $B$ such that
$$
AB = I
$$
Now, notice that when we perform matrix multiplication, we have
$$
\text{Col}_1(I) = e_1 = A \text{Col}_1(B) 
$$
and in general,
$$
\text{Col}_i(I) = e_i = A \text{Col}_i(B)
$$
So, we can solve the equations
$$
\begin{align*}
    A v_1 &= e_1 \implies LU v_1 &= e_1 \\
    A v_2 &= e_2 \implies LU v_2 &= e_2 \\
    &\vdots \\
    A v_n &= e_n \implies LU v_n &= e_n
\end{align*}
$$
each equation takes $O(n^2)$ FLOPs to solve, thus in total, we have $O(n^3)$ FLOPs to find the inverse of a matrix.

## Solving Special Linear Systems
Can we exploit matrix properties to more efficiently solve $Ax = b$? We will look at the following special cases:
- Symmetric systems $A = A^T$
- Positive definite systems $A$ satisfies $x^T A x > 0$ for all $x \neq 0$.
- Symmetric positive definite systems
- Banded systems
- Tri-diagonal systems
- General Sparse systems

### Symmetric Systems
Consider systems satisfying $A = A^T$. Clearly, such systems must also be square. Consider a new factorization $A = LDM^T$
<Theorem>
    <TheoremContent>
        If $A$ possesses an $LU$ factorization, then there exists unique unit lower triangular matrices $L$ and $M$ (unit triangular meaning that the diagonal entries are all $1$), and a diagonal matrix $D$, such that
        $$
        A = LDM^T
        $$
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Suppose we have $A = LU$, where $L$ is lower triangular and $U$ is upper triangular. Recall that $L$ is constructed from the identity matrix, so $L$ is already unit lower triangular. $U$ is constructed by applying Gaussian elimination to $A$, so $U$ is upper triangular, but not unit triangular. So we need to change $U$ so that it has 1s on the diagonal. How do we do this? We can multiply $U$ by a diagonal matrix $D$ that has the reciprocal of the diagonal entries of $U$. This is the general idea.

        Consider diagonal entries of $U$ and form a diagonal matrix $D$ with $d_{ii} = u_{ii}$ for $1 \leq i \leq n$. Then we have
        $$
        A = L DD^{-1} U
        $$
        The matrix $M^T = D^{-1}U$ is a unit upper triangular matrix, so $M$ is a unit lower triangular matrix, and we have 
        $$
        A = LDD^{-1}U = LDM^T
        $$
    </ProofContent>
</Proof>
The Flop count for this factorization is asymptotically unchanged from $LU$ factorization. This factorization is valid for both symmetric and non-symmetric matrices. But, if the matrix $A$ is symmetric, we find an advantage, by skipping redundant computation, and we can save roughly half the cost of $LU$ factorization.
<Theorem>
    <TheoremContent>
        Assume $A$ is invertible and has an $LU$ factorization. If $A$ is also symmetric, then $A = LDL^T$ (i.e. $M = L$).
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Let $A = LU$. Then $A = LDM^T$. We see that $M^{-1}AM^{-T}$ is a symmetric matrix since:
        $$
        \begin{align*}
            (M^{-1}AM^{-T})^T &= (M^{-T})^T A^T (M^{-1})^T \\
            &= M^{-1} A M^{-T}
        \end{align*}
        $$
        Now, we can write
        $$
        \begin{align*}
            M^{-1}AM^{-T} &= M^{-1}LDM^TM^{-T} \\
            &= M^{-1}LD
        \end{align*}
        $$
        Since $M^{-1}AM^{-T}$ is symmetric, we have that $M^{-1}LD$ is symmetric as well. $M^{-1}$L is the product of 2 lower triangular matrices, so it must also be lower triangular.

        Since $D$ is a diagonal matrix, $M^{-1}LD$ is a lower triangular matrix. Since it is both symmetric and lower triangular, it means that $M^{-1} LD$ is diagonal.

        We also know that $M^{-1}L$ is unit lower triangular (since both are lower unit triangular). On the other hand, $D$ is diagonal, so $M^{-1}L$ must be diagonal as well. This means that $M^{-1}L$ is the identity matrix, which gives us $M = L$.
    </ProofContent>
</Proof>

### Positive Definite Matrices
<Definition>
    <DefinitionName>Positive Definite</DefinitionName>
    <DefinitionContent>
        $A$ is positive definite if $x^T Ax > 0$ for all vectors $x \neq 0$.

        $f(x) = x^TAx$ is called a **quadratic form**, where $A$ contains the coefficients. If $A$ is positive definite, then $A$ is invertible and have strictly positive eigenvalues.

        In this course, we assume that positive definite matrices are symmetric.
    </DefinitionContent>
</Definition>

For example, for $x \in \mathbb R^2$, we have
$$
\begin{align*}
    \begin{bmatrix}
        x_1 & x_2
    \end{bmatrix} \begin{bmatrix}
        a & b \\
        b & c
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        x_2
    \end{bmatrix} &= ax_1^2 + 2bx_1x_2 + cx_2^2
\end{align*}
$$
If we are given a quadratic form $5x_1^2 + 7x_1x_2 + 12x_2^2$, we can just match the coefficients to recover the matrix $A$.

Even if $A$ is not symmetric, we can still define a symmetric matrix $B = \frac{1}{2}(A + A^T)$, then $B$ gives the same quadratic form of $A$.

Let's show that positive definiteness implies invertibility.
<Proof withName={true}>
    <ProofName>Positive Definiteness $\implies$ Invertibility</ProofName>
    <ProofContent>
        Suppose $A$ is positive definite. For contradiction, assume $A$ is not invertible. Then the nullspace of $A$ is not trivial. So there exists $v \neq 0$ such that $Av = 0$. Then we have
        $$
        \begin{align*}
            Av &= 0 \\
            v^T Av &= v^T 0 \\
            v^T Av = 0
        \end{align*}
        $$
        But since $A$ is positive definite, $v^T Av > 0$ for all $v \neq 0$, so we have a contradiction. Therefore, $A$ must be invertible.
    </ProofContent>
</Proof>
Now we show that positive definiteness implies that the eigenvalues are strictly positive.
<Proof withName={true}>
    <ProofName>Positive Definiteness $\implies$ $>0$ Eigenvalues</ProofName>
    <ProofContent>
        We proved above that positive definiteness implies invertibility. So $Ax = 0 = 0x$ has only the trivial solution, but 0 is not an eigenvalue of $A$. If $0$ is an eigenvalue, then there exists $v \neq 0$ where $Av = 0v = 0$, which contradicts the fact that $Ax = 0$ has only the trivial solution. Therefore, all eigenvalues of $A$ are non-zero.

        Assume that $\lambda$ is an eigenvalue that is $< 0$. Then there exists $v$ such that $Av = \lambda v$, then we have
        $$
        \begin{align*}
            v^T Av &= v^T \lambda v \\
            &= \lambda v^T v \\
            &= \lambda \|v\|^2
        \end{align*}
        $$
        In particular, $\lambda < 0$ and $\|v\|^2 > 0$, so $v^T Av < 0$, which contradicts the fact that $A$ is positive definite. Therefore, all eigenvalues of $A$ are $> 0$.
    </ProofContent>
</Proof>
We have a useful theorem:
<Theorem>
    <TheoremContent>
        If $A \in \mathbb R^{n \times n}$ is positive definite (P.D.), and $X \in \mathbb R^{n \times k}$ with $\text{rank}(A) = k \leq n$ (It has full rank), then $B = X^T AX$ is also P.D..
    </TheoremContent>
</Theorem>
Why is this useful? We can create positive definite matrices using existing positive definite matrices. 
<Proof>
    <ProofContent>
        Consider $z \in \mathbb R^k$. We want to show that $z^T Bz > 0$ for all $z \neq 0$. We have
        $$
        z^T Bz = z^T(X^T AX)z = (Xz)^T A (Xz)
        $$
        Let $x = Xz \in \mathbb R^n$. If $x \neq 0$, then $x^T Ax > 0$ since $A$ is P.D..

        If $x = 0$, then $Xz = 0$, which only happens if $z = 0$ since $X$ has full rank, so $z^T Bz > 0$ for all $z \neq 0$, and thus $B$ is P.D..
    </ProofContent>
</Proof>
<Definition>
    <DefinitionName>Principal Submatrix</DefinitionName>
    <DefinitionContent>
        A principal submatrix is a smaller matrix produced by deleting a set of rows and the corresponding columns (we remove the same row and column).

        For example, 
        $$
        A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
        $$ 
        has principal submatrix
        $$
        \begin{bmatrix} 1 & 2 \\ 4 & 5 \end{bmatrix}
        $$
        by removing the last row and last column.

        To construct a submatrix, we can multiply $A$ by an identity matrix $I$ that has the columns that we want to keep. Notice that
        $$
        \begin{align*}
            \begin{bmatrix}
                a & b & c \\
                d & e & f \\
                g & h & i
            \end{bmatrix} \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
                0 & 1
            \end{bmatrix} = \begin{bmatrix}
                a & c \\
                d & f \\
                g & i
            \end{bmatrix}
        \end{align*}
        $$
        So this removes the second column. If we want to remove the second row, we multiply $A$ on the left by $I^T$.
        $$
        \begin{align*}
            \begin{bmatrix}
                1 & 0 & 0\\
                0 & 0 & 1\\
            \end{bmatrix}\begin{bmatrix}
                a & b & c \\
                d & e & f \\
                g & h & i
            \end{bmatrix} \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
                0 & 1
            \end{bmatrix} = \begin{bmatrix}
                a & c \\
                g & i
            \end{bmatrix}
        \end{align*}
        $$
    </DefinitionContent>
</Definition>
<Corollary>
    <CorollaryContent>
        1. If $A$ is P.D., then all principal submatrices of $A$ are also P.D.. In particular, the diagonal entries are all positive (Note that each diagonal entry is a principal submatrix with all other rows/columns deleted).
        2. If $A$ is P.D., then diagonal $D$ of $A = LDM^T$ has strictly positive entries.
    </CorollaryContent>
</Corollary>
<Proof withName={true}>
    <ProofName>$A$ P.D. $\implies$ all principal submatrices P.D.</ProofName>
    <ProofContent>
        If we multiply $A$ by an identity matrix $I$, we get
        $$
        A \begin{bmatrix}
            e_1  & e_2  & \ldots  & e_n
        \end{bmatrix} = \begin{bmatrix}
            Ae_1  & Ae_2  & \ldots  & Ae_n
        \end{bmatrix}
        $$
        where $e_i$ is the $i$th column of the identity matrix. So, if we remove $e_i$ from $I$, then the resulting matrix has column $i$ removed. On the other hand, transposing $I$ and removing $e_i$, and we get $I^T A$, which removes row $i$.

        Now, note that the identity matrix with columns removed is still full rank since the columns are still linearly indepedent. So we can use the previous theorem that states that if $A$ is P.D., then $B = X^T AX$ ,where $X$ is full rank, is also P.D..
    </ProofContent>
</Proof>
<Proof withName={true}>
    <ProofName>$A$ P.D. $\implies$ diagonal entries of $D$ are positive</ProofName>
    <ProofContent>
        We have $A = LDM^T$. We let $X = L^{-T}$. Then
        $$
        \begin{align*}
            X^T AX &= L^{-1} A L^{-T} \\
            &= L^{-1} LDM^T L^{-T} \\
            &= DM^T L^{-T}
        \end{align*}
        $$
        We have that $X^T A X$ is P.D. (from the previous theorem), so $ DM^T L^{-T}$ is P.D. as well, so all its diagonal entries are positive.

        Since $M^T$ and $L^{-T}$ are upper unit triangular, $M^T L^{-T}$ is also upper unit triangular. Thus $\text{diag}(DM^T L^{-T}) = D$. So $D$ has positive diagonal entries.
    </ProofContent>
</Proof>

Now, under what condition can we write $A$ as $A = GG^T$? If entries of $D$ are all positive, then we can define
$$
D^{\frac{1}{2}} = \text{diag}(\sqrt{d_1}, \ldots, \sqrt{d_n})
$$
then we have
$$
\begin{align*}
    A &= LDL^T \\
    &= LD^{\frac{1}{2}} D^{\frac{1}{2}} L^T \\
    &= LD^{\frac{1}{2}} (D^{\frac{1}{2}})^T L^T \\
    &= LD^{\frac{1}{2}} (LD^{\frac{1}{2}})^T \\
    &= GG^T
\end{align*}
$$

<Theorem withName={true}>
    <TheoremName>Cholesky Factorization</TheoremName>
    <TheoremContent>
        If $A$ is P.D., then there exists a unique lower triangular matrix $G$ with positive diagonal entries such that $A = GG^T$.

        This is called the Cholesky factorization and $G$ is the Cholesky factor.
    </TheoremContent>
</Theorem>
** Insert Cholesky factorization example here**.

Here is the algorithm for Cholesky factorization:
```python
# k = 1, ..., n
for k in range(1, n + 1):
    a_kk = sqrt(a_kk)
    # i = k + 1, ..., n
    for i in range(k + 1, n + 1):
        a_ik = a_ik / a_kk
    # j = k + 1, ..., n
    for j in range(k + 1, n + 1):
        # i = j, ..., n
        for i in range(j, n + 1):
            a_ij = a_ij - a_ik * a_jk
```
Focusing on the innermost loop, we have one subtraction and one multiplication, so the cost is
$$
\sum^n_{k=1} \sum^n_{j = k + 1} \sum^n_{i=j} 2 = \frac{n^3}{3} + O(n^2)
$$
which is half of $LU$ factorization.

### Banded Systems
A banded matrix is a matrix where there are bands of non-zero entries at the top and bottom of the diagonal. Here are some examples
$$
\begin{bmatrix}
    9 & 5 & 0 & 0 \\
    0 & 3 & 7 & 0 \\
    0 & 0 & 1 & 4 \\
    0 & 0 & 0 & 2
\end{bmatrix} \qquad \begin{bmatrix}
    5 & 6 & 0 & 0 & 0 & 0 \\
    7 & 7 & 1 & 0 & 0 & 0 \\
    4 & 2 & 9 & 2 & 0 & 0 \\
    0 & 3 & 1 & 8 & 5 & 0 \\
    0 & 0 & 3 & 4 & 6 & 1 \\
    0 & 0 & 0 & 2 & 3 & 7
\end{bmatrix}
$$
We let $q$ be the number of bands on top of the diagonal, and $p$ be the number of bands below the diagonal. In the examples above, $q = 1, p = 0$ for the first matrix, and $q = 1, p = 2$ for the second matrix. More formally, we have the following.
<Definition>
    <DefinitionName>Bandwidth of Banded Matrix</DefinitionName>
    <DefinitionContent>
        $A$ has 
        - upper bandwidth $q$ if $a_{ij} = 0$ for $j > i + q$
        - lower bandwidth $p$ if $a_{ij} = 0$ for $i > j + p$
    </DefinitionContent>
</Definition>
Let's think about how we can efficiently store such matrices. One such way is storing it is by storing only the bands:
$$
\begin{bmatrix}
    9 & 5 & 0 & 0 \\
    0 & 3 & 7 & 0 \\
    0 & 0 & 1 & 4 \\
    0 & 0 & 0 & 2
\end{bmatrix} \rightarrow \begin{bmatrix}
    9 & 5 \\
    3 & 7 \\
    1 & 4 \\
    2 & 0
\end{bmatrix}
$$
But of course, this is not the only way, and we will not go into detail about this.
<Theorem>
    <TheoremContent>
        Let $A = LU$. If $A$ has upper bandwidth $q$ and lower bandwidth $p$, then $U$ has upper bandwidth $q$ and $L$ has lower bandwidth $p$.
    </TheoremContent>
</Theorem>
Given these properties of banded matrices, we have the following factoring algorithm:
```python
# k = 1, ..., n - 1
for k in range(1, n):
    # i = k + 1, ..., min(k + p, n)
    for i in range(k + 1, min(k + p, n) + 1):
        a_ik = a_ik / a_kk
    # i = k + 1, ..., min(k + p, n)
    for i in range(k + 1, min(k + p, n) + 1):
        # j = k + 1, ..., min(k + q, n)
        for j in range(k + 1, min(k + q, n) + 1):
            a_ij = a_ij - a_ik * a_kj
```
If $n >> p$ and $n >> q$ (meaning that there are not a lot of bands), then the cost is approximately $2npq$.

Comparing this to $\frac{2n^3}{3}$ for the naive LU factorization, band LU can be much faster.