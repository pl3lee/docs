import { Definition, DefinitionName, DefinitionContent, Corollary, CorollaryName, CorollaryContent, Example, ExampleName, ExampleContent, Problem, ProblemName, ProblemContent, Proposition, PropositionName, PropositionContent, Theorem, TheoremName, TheoremContent, Lemma, LemmaName, LemmaContent, Proof, ProofName, ProofContent } from "/components/math";
import { Steps, Callout } from 'nextra/components'

# CS 475: Computational Linear Algebra
## Introduction
### What is Numerical Linear Algebra?
Briefly, it is developing **efficient** algorithms to **accurately** solve linear algebra problems on a computer (with finite precision arithmetic). We will focus on the following topics:
- solving linear systems of equations.
- solving linear least squares problems.
- solving eigenvalue problems.
- performing singular value decompositions.

### Types of Algorithms
There are two types of algorithms:
- **Direct**: These algorithms compute the final solution with a finite sequence of arithmetic operations. For example, Gaussian elimination (LU factorization)
- **Iterative**: These algorithms start with an initial guess, and iteratively apply the same operation to get closer to the solution, until it is good enough. For example, Jacobi, Gauss-Seidel, Conjugate Gradient, and PageRank.

### Matrix Structure
There are two types of matrices:
- **Dense**: Most of the entries are non-zero. These are stored in $N \times N$ arrays, which we can manipulate normally.
- **Sparse**: Most entries are zero. Non-zero locations may exhibit patterns.

It is possible to exploit sparsity patterns/structure to save memory and/or flops. However, just because a matrix is sparse, it does not mean that it will be faster to solve. It depends on the sparsity pattern and the algorithm used. For example, even if we input a sparse matrix into a Gaussian elimination algorithm, it will still take the same amount of time.

### Factorization
A common theme is that we can express a given matrix as a product of other matrices. For example,
$$
A = BCD
$$
Some common factorizations are:
- LU factorization: $A = LU$
- QR factorization: $A = QR$
- Cholesky factorization: $A = LL^T$
- Singular Value Decomposition: $A = U \Sigma V^T$
- etc.

These factorizations have properties that we can exploit in algorithm design.

### Orthogonality
Notions of orthogonality will arise repeatedly in designing algorithms. Recall the following definitions for orthogonality.
<Definition withName={true}>
    <DefinitionName>Orthogonal Vectors</DefinitionName>
    <DefinitionContent>
        Two vectors $x$ and $y$ are orthogonal if $x^T y = 0$.
    </DefinitionContent>
</Definition>
<Definition withName={true}>
    <DefinitionName>Orthogonal Matrices</DefinitionName>
    <DefinitionContent>
        A real square matrix $Q$ is orthogonal if $Q^T Q = I$, or $Q^T = Q^{-1}$.
    </DefinitionContent>
</Definition>

### Linear Systems
This is the problem of solving $Ax = b$ for $x$ where $A \in \mathbb R^{n \times n}, x \in \mathbb R^n, b \in \mathbb R^n$. Doing it efficiently and accurately on large problems (in floating point arithmetic) can take substantial effort to get right.

For example, if we have the system $Ax = b$, then we could rearrange it to get $x = A^{-1}b$. However, computing the inverse of $A$ is potentially expensive and numerically unstable.

### Least Squares Problem
This is used for solving more general problems which may have
- Too many equations/constraints (over-determined).

    For example, $A \in \mathbb R^{30 \times 2}, b \in \mathbb R^{30}$, i.e. the matrix $A$ is tall and skinny.

- Too few equations/constraints (under-determined).

    For example, $A \in \mathbb R^{2 \times 30}, b \in \mathbb R^2$, i.e. the matrix $A$ is short and fat.

We aim to find an answer that is as good as possible. This is often done on data-fitting or regression problems.

If we want to solve $Ax = b$, rearranging gives $Ax - b = 0$. Therefore, we can minimize the norm $||Ax - b||_2$, where the two norm is defined as $||x||_2 = \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$. Notice that this is equivalent to minimizing $||Ax - b||_2^2$.

In regression problems, we may have $n$ datapoints $(x_1,y_1),\ldots,(x_n,y_n)$, and we want to find a line that is closest to these points. In the best case, a line is able to pass through all the points. In this case, we have the system of equations
$$
\begin{align*}
    y_1 &= ax_1 + c \\
    y_2 &= ax_2 + c \\
    &\vdots \\
    y_n &= ax_n + c
\end{align*}
$$
which can be written as $Ax = b$ where
$$
\underbrace{\begin{pmatrix}
    x_1 & 1 \\
    x_2 & 1 \\
    \vdots & \vdots \\
    x_n & 1
\end{pmatrix}}_{A} \underbrace{\begin{pmatrix}
    a \\
    c
\end{pmatrix}}_{x} = \underbrace{\begin{pmatrix}
    y_1 \\
    y_2 \\
    \vdots \\
    y_n
\end{pmatrix}}_{b}
$$
If we minimize $||Ax - b||_2^2$, then in the best case, we will get a line that passes through all the points. Even if we are not in the best case, we still get a line that is as close as possible to the points.

### Eigenvalue Problems
Find the eigenvalues $\lambda$ and eigenvectors $v$ of a $n \times n$ matrix $A$ such that
$$
Av = \lambda v
$$
What does this mean geometrically? Recall that matrices are linear transformations and it maps vectors by stretching, rotating, or reflecting them. Then eigenvectors are vectors such that it only gets stretched by a scalar factor, and the direction remains the same.

The eigenvalue problem is related to a factorization of $A$ into
$$
A = Q\Lambda Q^{-1}
$$
where $\Lambda$ is a diagonal matrix, and columns of $Q$ are eigenvectors of $A$.

We also have the definition of invariant:
<Definition>
    <DefinitionName>Invariant</DefinitionName>
    <DefinitionContent>
        Something that does not change under a transformation. So in the eigenvalue problem, the span of an eigenvector is invariant under $A$.
    </DefinitionContent>
</Definition>

### Singular Value Decomposition (SVD)
Given a general matrix $A$, how do we associate it with a square matrix? We can simply take $A^TA$ or $AA^T$, and they are square matrices. The SVD is a generalization of the eigenvalue problem that applies to all matrices. We solve for a factorization of a general $m \times n$ matrix $A$ such that
$$
A = U \Sigma V^T
$$
where
- $U, V$ are orthogonal matrices
- $\Sigma$ is diagonal with non-negative entries (called the singular values of $A$), which are found by taking the square root of the eigenvalues of $A^TA$ or $AA^T$.
 
## Background and LU Factorization
### Background
Here are some basic definitions.
<Definition>
    <DefinitionName>Vector Space</DefinitionName>
    <DefinitionContent>
        A vector space is a collection/set of vectors, together with addition and scalar multiplication.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Basis</DefinitionName>
    <DefinitionContent>
        A set of vectors $B$, is a basis for a vector space $V$ if
        - $B$ is linearly independent.
        - Every element of $V$ is a linear combination of elements of $B$.
    
    We can say that $B$ spans $V$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Range</DefinitionName>
    <DefinitionContent>
        The range of a given matrix $A$ is defined by
        $$
        \mathrm{range}(A) = \{Ax : x \in \mathbb R^n\}
        $$
        which is the space of vectors that can be generated by left-multiplying some vector $x$ with the matrix $A$.
    </DefinitionContent>
</Definition>
Note that we can view matrix-vector multiplication as taking a linear combination of $A$'s columns, where $x_i$'s gives the coefficients for each column:
$$
\begin{align*}
    \begin{pmatrix}
    y_1 \\ y_2 \\ \vdots \\ y_n
    \end{pmatrix} &= \begin{pmatrix}
    a_1 & a_2 & \cdots & a_n
    \end{pmatrix} \begin{pmatrix}
    x_1 \\ x_2 \\ \vdots \\ x_n
    \end{pmatrix} \\
    &= x_1a_1 + x_2a_2 + \cdots + x_na_n
\end{align*}
$$
where each $a_1, a_2, \ldots, a_n$ are the columns of $A$. Thus, all vectors in the range of $A$ can be expressed as linear combinations of columns of $A$. Therefore, range is also called the **column space**.
<Definition>
    <DefinitionName>Row Space</DefinitionName>
    <DefinitionContent>
        The row space is analogous to column space, which is the space of vectors that can be written as linear combinations of the rows of $A$. In terms of matrix multiplication, this is the space of vectors that can be generated by right-multiplying some vector $x$ with the matrix $A$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Nullspace</DefinitionName>
    <DefinitionContent>
        The nullspace or kernel of a matrix $A$ is defined by
        $$
        \mathrm{null}(A) = \{x : Ax = 0\}
        $$
        intuitively, it is the part of the vector space that gets mapped to the zero vector by $A$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Dimension of a Vector Space</DefinitionName>
    <DefinitionContent>
        If a vector space $V$ has a basis of $n$ elements, we say that the dimension of $V$, denoted by $\dim(V)$, is $n$. That is, $\dim(V)$ is the number of linearly independent vectors needed to span $V$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Rank</DefinitionName>
    <DefinitionContent>
        The column rank is the dimension of the column space of a matrix $A$. The row rank is the dimension of the row space of a matrix $A$. 

        It can be proven that column rank $=$ row rank. This is because $A$ can be factorized into $A = PEQ$, where $E = \begin{pmatrix} I & 0 \\ 0 & 0 \end{pmatrix}$, and $P, Q$ are invertible matrices. That is, $E$ is a diagonal matrix starting with $1$'s and ending with $0$'s. Then, the number of linearly independent columns of $A$ is the same as the number of linearly independent rows of $A$.

        Since both column rank and row rank are equal, we can simply refer to it as the rank of $A$.
    </DefinitionContent>
</Definition>
<Definition>
    <DefinitionName>Nullity</DefinitionName>
    <DefinitionContent>
        The dimension of the nullspace is the nullity of $A$. 
    </DefinitionContent>
</Definition>
<Theorem withName={true}>
    <TheoremName>Rank-Nullity Theorem</TheoremName>
    <TheoremContent>
        We have
        $$
        \mathrm{rank}(A) + \mathrm{nullity}(A) = n
        $$
        where $n$ is the number of columns of $A$.
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Let $r$ be the rank and $k$ be the nullity. Suppose that $A: V \to W$ where $V$ is $n$-dimensional and $W$ is $t$ dimensional. Suppose that the nullspace has basis $\{v_1,\ldots,v_k\}$, and suppose that $V$ has basis
        $$
        \{v_1,\ldots,v_k,v_{k+1},\ldots,v_n\}
        $$
        Then, applying $A$ to the basis vectors, we get the image of $A$:
        $$
        \{A(v_1),\ldots,A(v_k),A(v_{k+1}),\ldots,A(v_n)\} = \{Av_{k+1},\ldots,Av_n\}
        $$
        since $Av_i = 0$ for $i = 1,\ldots,k$. This set is linearly independent, and so the rank of $A$ is $n - k = r$. Therefore, we have
        $$
        r + k = n
        $$
    </ProofContent>
</Proof>
Note that a matrix is of full rank if the rank is equal to the minimum of the number of rows and columns. We have the following theorem:
<Theorem>
    <TheoremContent>
        A full rank matrix defines a one-to-one map, that is $Av_1 \neq Av_2$ for any $v_1 \neq v_2$.
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Suppose that $v_1 \neq v_2$ and for contradiction we have $Av_1 = Av_2$. Then $A(v_1 - v_2) = 0$, which implies that $v_1 - v_2 \in \mathrm{null}(A)$. But, since $A$ is full rank, $\mathrm{null}(A) = \{0\}$, and so $v_1 - v_2 = 0$, which is a contradiction.
    </ProofContent>
</Proof>
<Definition>
    <DefinitionName>Invertible</DefinitionName>
    <DefinitionContent>
        A square matrix of full rank is called invertible or nonsingular. The inverse of $A$ is denoted by $A^{-1}$, and is a unique matrix that satisfies
        $$
        AA^{-1} = A^{-1}A = I
        $$
        where $I$ is the identity matrix.
    </DefinitionContent>
</Definition>
We have the following useful properties of invertible matrices:
<Theorem>
    <TheoremContent>
        For a real square matrix $A \in \mathbb R^{n \times n}$, the following are equivalent:
        - $A$ is invertible.
        - $A$ is full rank, so $\mathrm{rank}(A) = n$.
        - $\mathrm{range}(A) = \mathbb R^n$.
        - $\mathrm{null}(A) = \{0\}$.
        - $A$ has no zero eigenvalues.
        - $A$ has no zero singular values.
        - $\det(A) \neq 0$.
    </TheoremContent>
</Theorem>
And we have some identies:
<Theorem withName={true}>
    <TheoremName>Invertible Matrix Identities</TheoremName>
    <TheoremContent>
        For invertible matrices $A$ and $B$:
        - $(AB)^{-1} = B^{-1}A^{-1}$
        - $(A^T)^{-1} = (A^{-1})^T = A^{-T}$
        - $B^{-1} = A^{-1} - B^{-1}(B - A)A^{-1}$
    </TheoremContent>
</Theorem>
Given a matrix $A$, and say you have already found its inverse. If you want to change $A$ slightly by adding $uv^T$, how does $A^{-1}$ change? Turns out we have a formula for this:
<Definition>
    <DefinitionName>Sherman-Morrison</DefinitionName>
    <DefinitionContent>
        If $1 + v^TA^{-1}u \neq 0$, then
        $$
        (A + uv^T)^{-1} = A^{-1} - \frac{A^{-1}uv^TA^{-1}}{1 + v^TA^{-1}u}
        $$
        for $A \in \mathbb R^{n \times n}$, $u,v \in \mathbb R^n$.
    </DefinitionContent>
</Definition>
Notice the denominator $1 + v^TA^{-1}u$. How is matrix division defined? Turns out that $v^T A^{-1} u \in \mathbb R$ is a scalar, and so we can divide by it. This formula allows us to update $A^{-1}$ without starting from scratch. This is called a rank-one update, since the matrix $uv^T$ has rank one. A generalization of this formula is the Sherman-Morrison-Woodbury formula. 
<Definition>
    <DefinitionName>Sherman-Morrison-Woodbury</DefinitionName>
    <DefinitionContent>
        If $A \in \mathbb R^{n \times n}$ is invertible, $U, V \in \mathbb R^{n \times k}$, then
        $$
        (A + UV^T)^{-1} = A^{-1} - A^{-1} - A^{-1}U(I + V^TA^{-1}U)^{-1}V^TA^{-1}
        $$
        That is, a rank $k$ modification to matrix $A$ yields a rank $k$ correction to $A^{-1}$.
    </DefinitionContent>
</Definition>

### LU Factorization
A lot of practical problems rely on solving systems of linear equations of the form $Ax = b$. We will start by looking at direct methods for $Ax = b$. We will strive to avoid constructing $A^{-1}$ itself, since this can be
- less efficient (in terms of operation counts).
- less accurate (incurs more round-off errors).
- more costly in terms of storage (depending on matrix sparsity)

We also define forward and backward errors. Suppose that we compute $y = f(x)$. We might get $\hat y = f(x)$. The forward error is $||\hat y - y||$. Then, we might also have that $\hat y$ is the exact solution for $\hat y = f(x + \Delta x)$. The backward error is $||\Delta x||$. 

Now, we can interpret Gaussian elimination as follows:
1. Factor matrix $A$ into $A = LU$, where $L$ and $U$ are triangular.
2. Solve $Ly = b$ for intermediate vector $y$.
3. Solve $Ux = y$ for $x$.

This works because solving $Ax = b$ is equivalent to solving $LUx = b$, which is equivalent to solving $Ly = b$ by letting $y = Ux$.

We have the following pseudocode for LU factorization
- $k = 1,\ldots, n$ **do** (iterating over all rows)
   - **for** $i = k+1,\ldots, n$ **do** (iterate over each row $i$ beneath row $k$)
      - mult $= \frac{a_{ik}}{a_{kk}}$ (determine row $i$'s multiplicative factor)
      - $a_{ik} = $ mult (store the multiplicative factor in the matrix)
      - **for** $j = k+1,\ldots, n$ **do** (iterate over each column for current row)
         - $a_{ij} = a_{ij} - $ mult $\times a_{kj}$ (update the matrix)
      - **end for**
   - **end for**
- **end for**

So, suppose $A$ row reduces to $A^{(4)}$ by performing 3 row operations. We can store these row operations in identity matrices $M^{(1)}, M^{(2)}, M^{(3)}$ such that
$$
A^{(4)} = M^{(3)}M^{(2)}M^{(1)}A
$$
Now, if we set $A^{(4)} = U$, where $U$ is an upper triangular matrix, then we have
$$
\begin{align*}
    A &=  (M^{(1)})^{-1}(M^{(2)})^{-1}(M^{(3)})^{-1}U \\
      &= L U
\end{align*}
$$
So, we have $L = (M^{(1)})^{-1}(M^{(2)})^{-1}(M^{(3)})^{-1}$. We define $L_j$ as the inverse of the matrix $M^{(j)}$.

$L_j$ can be obtained from $M^{(j)}$ be swapping the signs of the off-diagonal elements. For example, consider the matrix $M^{(3)} = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & -\frac{1}{2} & 1
\end{bmatrix}$. We can obtain $L_3$ by swapping the sign of the off-diagonal element, so $L_3 = \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & \frac{1}{2} & 1
\end{bmatrix}$.

Moreover, $L$ can be obtained from all $L_j$'s by placing all of the off-diagonal elements in the matrices into the corresponding position in $L$. So, consider matrices
$$
\begin{align*}
    M^{(1)} &= \begin{bmatrix}
        1 & 0 & 0 \\
        0.3 & 1 & 0 \\
        0 & 0 & 1
        \end{bmatrix} \\
    M^{(2)} &= \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        -0.5 & 0 & 1
        \end{bmatrix} \\
    M^{(3)} &= \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 25 & 1
        \end{bmatrix}
\end{align*}
$$
Then, we have
$$
\begin{align*}
    L &= L_1L_2L_3 \\
    &= (M^{(1)})^{-1}(M^{(2)})^{-1}(M^{(3)})^{-1} \\
    &= \begin{bmatrix}
        1 & 0 & 0 \\
        -0.3 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix} \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0.5 & 0 & 1
    \end{bmatrix} \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & -25 & 1
    \end{bmatrix} \\
    &= \begin{bmatrix}
        1 & 0 & 0 \\
        -0.3 & 1 & 0 \\
        0.5 & -25 & 1
    \end{bmatrix}
\end{align*}
$$

Let's try to solve $\begin{bmatrix}
    1 & 1 & 1 \\
    1 & -2 & 2 \\
    1 & 2 & -1
\end{bmatrix} \begin{bmatrix}
    x_0 \\ x_1 \\ x_2
\end{bmatrix} = \begin{bmatrix}
    0 \\ 4 \\ 2
\end{bmatrix}$ for the vector $x$.

First, we will factor $A$ into $LU$.
1. We start with the intial matrix $A$ and the $L$ the identity matrix
    $$
    A = \begin{bmatrix}
        1 & 1 & 1 \\
        1 & -2 & 2 \\
        1 & 2 & -1
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
    $$
2. We row reduce by $R_2 \to R_2 - R_1$ to obtain
    $$
    A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        1 & 2 & -1
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        0 & 0 & 1
    \end{bmatrix}
    $$
3. We row reduce by $R_3 \to R_3 - R_1$ to obtain
    $$
    A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        0 & 1 & -2
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        1 & 0 & 1
    \end{bmatrix}
    $$
4. We row reduce by $R_3 \to R_3 - \left(-\frac{1}{3}\right)R_2$ to obtain
    $$
    U = A = \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        0 & 0 & -\frac{5}{3}
    \end{bmatrix}, L = \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        1 & -\frac{1}{3} & 1
    \end{bmatrix}
    $$

Then, given $b = \begin{bmatrix}
    0 \\ 4 \\ 2
\end{bmatrix}$, we can find $x$ such that $Ax = b$ using forward/backward solve. We first do forward solve $Ly = b$ for $y$.
$$
\begin{align*}
    \begin{bmatrix}
        1 & 0 & 0 \\
        1 & 1 & 0 \\
        1 & -\frac{1}{3} & 1
    \end{bmatrix} \begin{bmatrix}
        y_1 \\ y_2 \\ y_3
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 4 \\ 2
    \end{bmatrix} \\
    \begin{bmatrix}
        y_1 \\ y_2 \\ y_3
    \end{bmatrix} = \begin{bmatrix}
    0 \\ 4 \\ \frac{10}{3}
    \end{bmatrix}
\end{align*}
$$
Why is it called forward solve? Because we solve $y_1$ first, then $y_2$, and finally $y_3$. Now, we do backward solve $Ux = y$ for $x$.
$$
\begin{align*}
    \begin{bmatrix}
        1 & 1 & 1 \\
        0 & -3 & 1 \\
        0 & 0 & -\frac{5}{3}
    \end{bmatrix} \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
    \end{bmatrix} = \begin{bmatrix}
        0 \\ 4 \\ \frac{10}{3}
    \end{bmatrix} \\
    \begin{bmatrix}
        x_1 \\ x_2 \\ x_3
    \end{bmatrix} = \begin{bmatrix}
        4 \\ -2 \\ -2
    \end{bmatrix}
\end{align*}
$$

So, the solution to the system of equations is $x = \begin{bmatrix}
    4 \\ -2 \\ -2
\end{bmatrix}$.

Now, why is two matrix solves better than one? It is because we can often reuse the factorization to solve for different $b$'s. So, we have the following pseudo code:
```python
# Forward solve

# loops for i = 1,..., n
for i in range(1, n + 1):
    y_i = b_i
    # loops for j = 1,..., i - 1
    for j in range(1, i):
        y_i = y_i - l_ij * y_j
```

```python
# Backward solve

# loops for i = n,..., 1
for i in range(n, 0, -1):
    x_i = y_i
    # loops for j = i + 1,..., n
    for j in range(i + 1, n + 1):
        x_i = x_i - u_ij * x_j
    x_i = x_i / u_ii
```
We want to know the asymptotic cost to solve a system of size $n$. We will measure the cost in total FLOPs: floating point operations. This is approximated as the number of additions and multiplications. The cost of factorization is
$$
\sum^n_{k=1} \left(\sum^n_{i = k + 1}\left(1 + \sum^n_{j=k+1} 2\right)\right) = \frac{2n^3}{3} + O(n^2)
$$
where $\sum^n_{i=k+1}$ is the middle loop, and $\sum^n_{j=k+1}$ is the innermost loop. In the inner loop, there are 2 operations, and in the middle loop, there is 1 operation.

So LU factorization costs approximately $O(n^3)$ FLOPs. What about forward and backward solves? Counting gives $n^2 + O(n)$ FLOPs for each triangular solve. Thus the total is $O(n^3)$. For large $n$, the cost of factorization is dominant. Then, given a factorization, solving an additional RHS is cheap, only $O(n^2)$.

### Finding the Inverse
Given a matrix $A$, how might we construct $A^{-1}$, if we really wanted to? We define $e_i$ to be the $i$th column in the identity matrix. Suppose that we let
$$
A^{-1} = \begin{bmatrix}
    v_1 & | & v_2 & | & \cdots & | & v_n
\end{bmatrix}
$$
We can then factor $A = LU$. Then, notice that we are finding $B$ such that
$$
AB = I
$$
Now, notice that when we perform matrix multiplication, we have
$$
\text{Col}_1(I) = e_1 = A \text{Col}_1(B) 
$$
and in general,
$$
\text{Col}_i(I) = e_i = A \text{Col}_i(B)
$$
So, we can solve the equations
$$
\begin{align*}
    A v_1 &= e_1 \implies LU v_1 &= e_1 \\
    A v_2 &= e_2 \implies LU v_2 &= e_2 \\
    &\vdots \\
    A v_n &= e_n \implies LU v_n &= e_n
\end{align*}
$$
each equation takes $O(n^2)$ FLOPs to solve, thus in total, we have $O(n^3)$ FLOPs to find the inverse of a matrix.

## Solving Special Linear Systems
Can we exploit matrix properties to more efficiently solve $Ax = b$? We will look at the following special cases:
- Symmetric systems $A = A^T$
- Positive definite systems $A$ satisfies $x^T A x > 0$ for all $x \neq 0$.
- Symmetric positive definite systems
- Banded systems
- Tri-diagonal systems
- General Sparse systems

### Symmetric Systems
Consider systems satisfying $A = A^T$. Clearly, such systems must also be square. Consider a new factorization $A = LDM^T$
<Theorem>
    <TheoremContent>
        If $A$ possesses an $LU$ factorization, then there exists unique unit lower triangular matrices $L$ and $M$ (unit triangular meaning that the diagonal entries are all $1$), and a diagonal matrix $D$, such that
        $$
        A = LDM^T
        $$
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Suppose we have $A = LU$, where $L$ is lower triangular and $U$ is upper triangular. Recall that $L$ is constructed from the identity matrix, so $L$ is already unit lower triangular. $U$ is constructed by applying Gaussian elimination to $A$, so $U$ is upper triangular, but not unit triangular. So we need to change $U$ so that it has 1s on the diagonal. How do we do this? We can multiply $U$ by a diagonal matrix $D$ that has the reciprocal of the diagonal entries of $U$. This is the general idea.

        Consider diagonal entries of $U$ and form a diagonal matrix $D$ with $d_{ii} = u_{ii}$ for $1 \leq i \leq n$. Then we have
        $$
        A = L DD^{-1} U
        $$
        The matrix $M^T = D^{-1}U$ is a unit upper triangular matrix, so $M$ is a unit lower triangular matrix, and we have 
        $$
        A = LDD^{-1}U = LDM^T
        $$
    </ProofContent>
</Proof>
The Flop count for this factorization is asymptotically unchanged from $LU$ factorization. This factorization is valid for both symmetric and non-symmetric matrices. But, if the matrix $A$ is symmetric, we find an advantage, by skipping redundant computation, and we can save roughly half the cost of $LU$ factorization.
<Theorem>
    <TheoremContent>
        Assume $A$ is invertible and has an $LU$ factorization. If $A$ is also symmetric, then $A = LDL^T$ (i.e. $M = L$).
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        Let $A = LU$. Then $A = LDM^T$. We see that $M^{-1}AM^{-T}$ is a symmetric matrix since:
        $$
        \begin{align*}
            (M^{-1}AM^{-T})^T &= (M^{-T})^T A^T (M^{-1})^T \\
            &= M^{-1} A M^{-T}
        \end{align*}
        $$
        Now, we can write
        $$
        \begin{align*}
            M^{-1}AM^{-T} &= M^{-1}LDM^TM^{-T} \\
            &= M^{-1}LD
        \end{align*}
        $$
        Since $M^{-1}AM^{-T}$ is symmetric, we have that $M^{-1}LD$ is symmetric as well. $M^{-1}$L is the product of 2 lower triangular matrices, so it must also be lower triangular.

        Since $D$ is a diagonal matrix, $M^{-1}LD$ is a lower triangular matrix. Since it is both symmetric and lower triangular, it means that $M^{-1} LD$ is diagonal.

        We also know that $M^{-1}L$ is unit lower triangular (since both are lower unit triangular). On the other hand, $D$ is diagonal, so $M^{-1}L$ must be diagonal as well. This means that $M^{-1}L$ is the identity matrix, which gives us $M = L$.
    </ProofContent>
</Proof>

### Positive Definite Matrices
<Definition>
    <DefinitionName>Positive Definite</DefinitionName>
    <DefinitionContent>
        $A$ is positive definite if $x^T Ax > 0$ for all vectors $x \neq 0$.

        $f(x) = x^TAx$ is called a **quadratic form**, where $A$ contains the coefficients. If $A$ is positive definite, then $A$ is invertible and have strictly positive eigenvalues.

        In this course, we assume that positive definite matrices are symmetric.
    </DefinitionContent>
</Definition>

For example, for $x \in \mathbb R^2$, we have
$$
\begin{align*}
    \begin{bmatrix}
        x_1 & x_2
    \end{bmatrix} \begin{bmatrix}
        a & b \\
        b & c
    \end{bmatrix} \begin{bmatrix}
        x_1 \\
        x_2
    \end{bmatrix} &= ax_1^2 + 2bx_1x_2 + cx_2^2
\end{align*}
$$
If we are given a quadratic form $5x_1^2 + 7x_1x_2 + 12x_2^2$, we can just match the coefficients to recover the matrix $A$.

Even if $A$ is not symmetric, we can still define a symmetric matrix $B = \frac{1}{2}(A + A^T)$, then $B$ gives the same quadratic form of $A$.

Let's show that positive definiteness implies invertibility.
<Proof withName={true}>
    <ProofName>Positive Definiteness $\implies$ Invertibility</ProofName>
    <ProofContent>
        Suppose $A$ is positive definite. For contradiction, assume $A$ is not invertible. Then the nullspace of $A$ is not trivial. So there exists $v \neq 0$ such that $Av = 0$. Then we have
        $$
        \begin{align*}
            Av &= 0 \\
            v^T Av &= v^T 0 \\
            v^T Av = 0
        \end{align*}
        $$
        But since $A$ is positive definite, $v^T Av > 0$ for all $v \neq 0$, so we have a contradiction. Therefore, $A$ must be invertible.
    </ProofContent>
</Proof>
Now we show that positive definiteness implies that the eigenvalues are strictly positive.
<Proof withName={true}>
    <ProofName>Positive Definiteness $\implies$ $>0$ Eigenvalues</ProofName>
    <ProofContent>
        We proved above that positive definiteness implies invertibility. So $Ax = 0 = 0x$ has only the trivial solution, but 0 is not an eigenvalue of $A$. If $0$ is an eigenvalue, then there exists $v \neq 0$ where $Av = 0v = 0$, which contradicts the fact that $Ax = 0$ has only the trivial solution. Therefore, all eigenvalues of $A$ are non-zero.

        Assume that $\lambda$ is an eigenvalue that is $< 0$. Then there exists $v$ such that $Av = \lambda v$, then we have
        $$
        \begin{align*}
            v^T Av &= v^T \lambda v \\
            &= \lambda v^T v \\
            &= \lambda \|v\|^2
        \end{align*}
        $$
        In particular, $\lambda < 0$ and $\|v\|^2 > 0$, so $v^T Av < 0$, which contradicts the fact that $A$ is positive definite. Therefore, all eigenvalues of $A$ are $> 0$.
    </ProofContent>
</Proof>
We have a useful theorem:
<Theorem>
    <TheoremContent>
        If $A \in \mathbb R^{n \times n}$ is positive definite (P.D.), and $X \in \mathbb R^{n \times k}$ with $\text{rank}(A) = k \leq n$ (It has full rank), then $B = X^T AX$ is also P.D..
    </TheoremContent>
</Theorem>
Why is this useful? We can create positive definite matrices using existing positive definite matrices. 
<Proof>
    <ProofContent>
        Consider $z \in \mathbb R^k$. We want to show that $z^T Bz > 0$ for all $z \neq 0$. We have
        $$
        z^T Bz = z^T(X^T AX)z = (Xz)^T A (Xz)
        $$
        Let $x = Xz \in \mathbb R^n$. If $x \neq 0$, then $x^T Ax > 0$ since $A$ is P.D..

        If $x = 0$, then $Xz = 0$, which only happens if $z = 0$ since $X$ has full rank, so $z^T Bz > 0$ for all $z \neq 0$, and thus $B$ is P.D..
    </ProofContent>
</Proof>
<Definition>
    <DefinitionName>Principal Submatrix</DefinitionName>
    <DefinitionContent>
        A principal submatrix is a smaller matrix produced by deleting a set of rows and the corresponding columns (we remove the same row and column).

        For example, 
        $$
        A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \\ 7 & 8 & 9 \end{bmatrix}
        $$ 
        has principal submatrix
        $$
        \begin{bmatrix} 1 & 2 \\ 4 & 5 \end{bmatrix}
        $$
        by removing the last row and last column.

        To construct a submatrix, we can multiply $A$ by an identity matrix $I$ that has the columns that we want to keep. Notice that
        $$
        \begin{align*}
            \begin{bmatrix}
                a & b & c \\
                d & e & f \\
                g & h & i
            \end{bmatrix} \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
                0 & 1
            \end{bmatrix} = \begin{bmatrix}
                a & c \\
                d & f \\
                g & i
            \end{bmatrix}
        \end{align*}
        $$
        So this removes the second column. If we want to remove the second row, we multiply $A$ on the left by $I^T$.
        $$
        \begin{align*}
            \begin{bmatrix}
                1 & 0 & 0\\
                0 & 0 & 1\\
            \end{bmatrix}\begin{bmatrix}
                a & b & c \\
                d & e & f \\
                g & h & i
            \end{bmatrix} \begin{bmatrix}
                1 & 0 \\
                0 & 0 \\
                0 & 1
            \end{bmatrix} = \begin{bmatrix}
                a & c \\
                g & i
            \end{bmatrix}
        \end{align*}
        $$
    </DefinitionContent>
</Definition>
<Corollary>
    <CorollaryContent>
        1. If $A$ is P.D., then all principal submatrices of $A$ are also P.D.. In particular, the diagonal entries are all positive (Note that each diagonal entry is a principal submatrix with all other rows/columns deleted).
        2. If $A$ is P.D., then diagonal $D$ of $A = LDM^T$ has strictly positive entries.
    </CorollaryContent>
</Corollary>
<Proof withName={true}>
    <ProofName>$A$ P.D. $\implies$ all principal submatrices P.D.</ProofName>
    <ProofContent>
        If we multiply $A$ by an identity matrix $I$, we get
        $$
        A \begin{bmatrix}
            e_1  & e_2  & \ldots  & e_n
        \end{bmatrix} = \begin{bmatrix}
            Ae_1  & Ae_2  & \ldots  & Ae_n
        \end{bmatrix}
        $$
        where $e_i$ is the $i$th column of the identity matrix. So, if we remove $e_i$ from $I$, then the resulting matrix has column $i$ removed. On the other hand, transposing $I$ and removing $e_i$, and we get $I^T A$, which removes row $i$.

        Now, note that the identity matrix with columns removed is still full rank since the columns are still linearly indepedent. So we can use the previous theorem that states that if $A$ is P.D., then $B = X^T AX$ ,where $X$ is full rank, is also P.D..
    </ProofContent>
</Proof>
<Proof withName={true}>
    <ProofName>$A$ P.D. $\implies$ diagonal entries of $D$ are positive</ProofName>
    <ProofContent>
        We have $A = LDM^T$. We let $X = L^{-T}$. Then
        $$
        \begin{align*}
            X^T AX &= L^{-1} A L^{-T} \\
            &= L^{-1} LDM^T L^{-T} \\
            &= DM^T L^{-T}
        \end{align*}
        $$
        We have that $X^T A X$ is P.D. (from the previous theorem), so $ DM^T L^{-T}$ is P.D. as well, so all its diagonal entries are positive.

        Since $M^T$ and $L^{-T}$ are upper unit triangular, $M^T L^{-T}$ is also upper unit triangular. Thus $\text{diag}(DM^T L^{-T}) = D$. So $D$ has positive diagonal entries.
    </ProofContent>
</Proof>

Now, under what condition can we write $A$ as $A = GG^T$? If entries of $D$ are all positive, then we can define
$$
D^{\frac{1}{2}} = \text{diag}(\sqrt{d_1}, \ldots, \sqrt{d_n})
$$
then we have
$$
\begin{align*}
    A &= LDL^T \\
    &= LD^{\frac{1}{2}} D^{\frac{1}{2}} L^T \\
    &= LD^{\frac{1}{2}} (D^{\frac{1}{2}})^T L^T \\
    &= LD^{\frac{1}{2}} (LD^{\frac{1}{2}})^T \\
    &= GG^T
\end{align*}
$$

<Theorem withName={true}>
    <TheoremName>Cholesky Factorization</TheoremName>
    <TheoremContent>
        If $A$ is P.D., then there exists a unique lower triangular matrix $G$ with positive diagonal entries such that $A = GG^T$.

        This is called the Cholesky factorization and $G$ is the Cholesky factor.
    </TheoremContent>
</Theorem>
** Insert Cholesky factorization example here**.

Here is the algorithm for Cholesky factorization:
```python
# k = 1, ..., n
for k in range(1, n + 1):
    a_kk = sqrt(a_kk)
    # i = k + 1, ..., n
    for i in range(k + 1, n + 1):
        a_ik = a_ik / a_kk
    # j = k + 1, ..., n
    for j in range(k + 1, n + 1):
        # i = j, ..., n
        for i in range(j, n + 1):
            a_ij = a_ij - a_ik * a_jk
```
Focusing on the innermost loop, we have one subtraction and one multiplication, so the cost is
$$
\sum^n_{k=1} \sum^n_{j = k + 1} \sum^n_{i=j} 2 = \frac{n^3}{3} + O(n^2)
$$
which is half of $LU$ factorization.

### Banded Systems
A banded matrix is a matrix where there are bands of non-zero entries at the top and bottom of the diagonal. Here are some examples
$$
\begin{bmatrix}
    9 & 5 & 0 & 0 \\
    0 & 3 & 7 & 0 \\
    0 & 0 & 1 & 4 \\
    0 & 0 & 0 & 2
\end{bmatrix} \qquad \begin{bmatrix}
    5 & 6 & 0 & 0 & 0 & 0 \\
    7 & 7 & 1 & 0 & 0 & 0 \\
    4 & 2 & 9 & 2 & 0 & 0 \\
    0 & 3 & 1 & 8 & 5 & 0 \\
    0 & 0 & 3 & 4 & 6 & 1 \\
    0 & 0 & 0 & 2 & 3 & 7
\end{bmatrix}
$$
We let $q$ be the number of bands on top of the diagonal, and $p$ be the number of bands below the diagonal. In the examples above, $q = 1, p = 0$ for the first matrix, and $q = 1, p = 2$ for the second matrix. More formally, we have the following.
<Definition>
    <DefinitionName>Bandwidth of Banded Matrix</DefinitionName>
    <DefinitionContent>
        $A$ has 
        - upper bandwidth $q$ if $a_{ij} = 0$ for $j > i + q$
        - lower bandwidth $p$ if $a_{ij} = 0$ for $i > j + p$
    </DefinitionContent>
</Definition>
Let's think about how we can efficiently store such matrices. One such way is storing it is by storing only the bands:
$$
\begin{bmatrix}
    9 & 5 & 0 & 0 \\
    0 & 3 & 7 & 0 \\
    0 & 0 & 1 & 4 \\
    0 & 0 & 0 & 2
\end{bmatrix} \rightarrow \begin{bmatrix}
    9 & 5 \\
    3 & 7 \\
    1 & 4 \\
    2 & 0
\end{bmatrix}
$$
But of course, this is not the only way, and we will not go into detail about this.
<Theorem>
    <TheoremContent>
        Let $A = LU$. If $A$ has upper bandwidth $q$ and lower bandwidth $p$, then $U$ has upper bandwidth $q$ and $L$ has lower bandwidth $p$.
    </TheoremContent>
</Theorem>
Given these properties of banded matrices, we have the following factoring algorithm:
```python
# k = 1, ..., n - 1
for k in range(1, n):
    # i = k + 1, ..., min(k + p, n)
    for i in range(k + 1, min(k + p, n) + 1):
        a_ik = a_ik / a_kk
    # i = k + 1, ..., min(k + p, n)
    for i in range(k + 1, min(k + p, n) + 1):
        # j = k + 1, ..., min(k + q, n)
        for j in range(k + 1, min(k + q, n) + 1):
            a_ij = a_ij - a_ik * a_kj
```
If $n >> p$ and $n >> q$ (meaning that there are not a lot of bands), then the cost is approximately $2npq$.

Comparing this to $\frac{2n^3}{3}$ for the naive LU factorization, band LU can be much faster.

## General Sparse Matrices and Finite Differences for PDEs
Patterns other than simple bands are also very common.

For many problems, the max number of non-zero entries in a row is constant, i.e. the total number of non-zero entries in a row is $O(n)$. Thus for storage, we want to store only the non-zero entries.

Then for GE/LU, the main cost is the row subtraction:
$$
a_{ij} = a_{ij} - \frac{a_{ij} a_{kj}}{a_{kk}}
$$
Since most entries are zero in a sparse matrix, out algorithms should skip operating on them.

### Sparse Matrix Storage
We look at Compressed Row Storage or Compressed Sparse Row (CRS or CSR). This is best illustrated using an example.
$$
\begin{bmatrix}
    2 & 0 & 5 & 0 \\
    0 & 3 & 0 & 0 \\
    0 & 6 & -3 & 0 \\
    0 & 0 & 10 & -2
\end{bmatrix}
$$
We first store the non-zero entries in a val vector:
$$
\text{val} = \begin{bmatrix}
    2 & 5 & 3 & 6 & -3 & 10 & -2
\end{bmatrix}
$$
Then we store the column indices of the non-zero entries in a colInd vector:
$$
\text{colInd} = \begin{bmatrix}
    1 & 3 & 2 & 2 & 3 & 3 & 4
\end{bmatrix}
$$
So, the entry $2$ is in the first column, $5$ is in the third column, etc.

Finally, we store the row pointers in a rowPtr vector:
$$
\text{rowPtr} = \begin{bmatrix}
    1 & 3 & 4 & 6
\end{bmatrix}
$$
This stores the index in the val vector of where the row starts (note that we are using 1-based indexing).

### LU on General Sparse Matrices - Fill-in
Note that even if $A$ is sparse, its factorization may not be sparse. Row reduction can turn zeros into non-zeros, this is called fill-in.

A classic example is the arrow matrix:
$$
\begin{bmatrix}
    \times & \times & \times & \times & \times \\
    \times & \times &  &  &  \\
    \times &  & \times &  &  \\
    \times &  &  & \times &  \\
    \times &  &  &  & \times
\end{bmatrix}
$$
The LU factorization of this matrix is fully dense.

However, if we reorder the rows and columns by switching the first and last row, and first and last column, we get
$$
\begin{bmatrix}
    \times &  &  &  & \times \\
     & \times &  &  & \times \\
     &  & \times &  & \times \\
     &  &  & \times & \times \\
    \times & \times & \times & \times & \times
\end{bmatrix} = \begin{bmatrix}
    1 & & & &  \\
    & 1 & & &  \\
    & & 1 & &  \\
    & & & 1 &  \\
    \times & \times & \times & \times & 1
\end{bmatrix}\begin{bmatrix}
    \times &  &  &  & \times \\
     & \times &  &  & \times \\
     &  & \times &  & \times \\
     &  &  & \times & \times \\
     &  &  &  & \times
\end{bmatrix}
$$
### Application Problem: Heat Conduction
Read the textbook

## Graph Structure of Matrices and Matrix Re-ordering
Idea: Suppose that we have $Ax = b$, where $A$ is sparse. We can reorder the rows of columns of $A$ to obtain $A'$ and $b'$ such that $A' x = b'$ gives the solution to the original problem, and $A'$ has less fill-ins in Gaussian elimination.

However, in general, the problem of giving an algorithm for reordering matrix in such a way that has the least fill-ins is an NP-hard problem.

First, let's show how we can represent the graph structure of a matrix. We have the matrices
$$
\begin{bmatrix}
    0 & 1 & 0 & 1 \\
    1 & 0 & 1 & 1 \\
    0 & 1 & 0 & 1 \\
    1 & 1 & 1 & 0
\end{bmatrix} \quad \begin{bmatrix}
    0 & 1 & 1 & 1 \\
    1 & 0 & 1 & 1 \\
    1 & 1 & 0 & 0 \\
    1 & 1 & 0 & 0
\end{bmatrix}
$$
We can label each column $1, 2, 3, 4$, and each row $1, 2, 3, 4$ starting from the left top corner. We can represent the matrix as a graph where row $i$ is a neighbour of column $j$ if the entry $a_{ij} \neq 0$. So, the graph of the matrix above is

![graph](/images/school/spring-2024/cs475/5-1.png)

We can see that if we reorder the first and fourth columns and rows, then we have the same graph structure. We denote the graph of $A$ $G(A)$.

Remarks:
- This is most useful when the matrix is sparse.
- Note that we exclude drawing self-loops (i.e. when $i = j$) in the graph.
- We consider mostly undirected symmetric graphs in this course.


Let's consider the relationship between factorization of sparse matrices, fill-in, and the graph structure.
- When we pivot with node $i$, we eliminate all edges incident to node $i$.
- New edges $jk$ are added if there were edges $ij$ and $ik$. That is, new edges bewteen all node pairs connected to $i$ in the old graph (this correponds to fill-ins).
- So ideally, we want the node in the top left entry to be a node that has the least neighbours.

For example, consider the arrow matrix and we perform a row reduction:
$$
\begin{bmatrix}
    \times & \times & \times & \times & \times \\
    \times & \times & & & \\
    \times & & \times & & \\
    \times & & & \times & \\
    \times & & & & \times
\end{bmatrix} \rightarrow \begin{bmatrix}
    \times & \times & \times & \times & \times \\
     & \times & \times &\times & \times\\
     & \times& \times &\times & \times\\
    & \times& \times& \times & \times\\
    & \times& \times& \times & \times
\end{bmatrix}
$$
When we eliminate the first column using the first diagonal entry as a pivot, we remove node 1 (which originally connects to all other nodes), and creates edges between all other nodes. As we can see, we have a lot of fill-ins.

- The ordering (numbering) of the nodes/variables impacts the matrix layout, but not its graph or the solution.
- The graph structure of a symmetric matrix is clearly unchanged by just renumbering its nodes.
- However, different matrices with the same graph can suffer vastly different levels of fill-in during factorization.
- Our goal of matrix reordering is to renumber the graph nodes to produce a matrix that minimizes fill-in during factorization.

Reordering a matrix can be written mathematically in terms of permutation matrices. A permutation matrix is the identity matrix $I$ with some rows/columns swapped.
- Permuting the rows is equivalent to multiplying $A$ by a permutation matrix $P$ on the left: $PA$
$$
\begin{bmatrix}
    0 & 1 & 0 \\
    0 & 0 & 1 \\
    1 & 0 & 0
\end{bmatrix} \begin{bmatrix}
    3 & 2 & 5 \\
    2 & 4 & 1 \\
    5 & 1 & 3
\end{bmatrix} = \begin{bmatrix}
    2 & 4 & 1 \\
    5 & 1 & 3 \\
    3 & 2 & 5
\end{bmatrix}
$$
We can see that the first row of the original matrix is the third row of the permuted matrix, second row of the original matrix is the first row of the permuted matrix, and the third row of the original matrix is the second row of the permuted matrix.
- Similarly, permuting the columns is equivalent to multiplying $A$ by a permutation matrix $Q$ on the right: $AQ$
$$
\begin{bmatrix}
    3 & 2 & 5 \\
    2 & 4 & 1 \\
    5 & 1 & 3
\end{bmatrix} \begin{bmatrix}

    0 & 1 & 0 \\
    0 & 0 & 1 \\
    1 & 0 & 0
\end{bmatrix} = \begin{bmatrix}
    5 & 3 & 2\\
    1 & 2 & 4\\
    3 & 5 & 1
\end{bmatrix}
$$
- Of course, we can permute the rows and the columns simultaneously: $PAQ$.

<Theorem>
    <TheoremContent>
        A permutation matrix is invertible.
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        A permutation matrix is full rank, so it is invertible. It's inverse is just its transpose, i.e. $P P^T = I$.
    </ProofContent>
</Proof>

Now, how do we reorder $b$ if we permute $A$ to $\tilde A = PAQ$? We have
$$
\begin{align*}
    Ax &= b \\
    A(QQ^T)x &= b \\
    AQ (Q^Tx) &= b \\
    PAQ (Q^T x) &= Pb \\
    \tilde A \tilde x &= \tilde b
\end{align*}
$$
so $\tilde x = Q^T x$ and $\tilde b = Pb$. Then, by solving $\tilde A \tilde x = \tilde b$, we can recover $x$ by $x = Q \tilde x$.

## Matrix Re-ordering
### Symmetric Permutations
The special case of symmetric permutation is of particular importance. A symmetric permutation is when we replace $A$ with $PAP^T$, i.e. we permute the rows and columns in the same way.


### Level Sets/Cuthill-McKee
#### Cuthill-Mckee
$S_i$ is the group of nodes that are the same graph distance from some starting point. For example,
- $S_1$ is the starting node
- $S_2$ is the set of all immediate neighbours of the node in $S_1$
- $S_3$ is the set of all immediate neighbours of the node in $S_2$ that are not in $S_1$
- and so on.

Here is an example:

![level sets](/images/school/spring-2024/cs475/6-1.png)

This is basically a breadth-first search. Now, how do we order nodes within each level set? In Cuthill-McKee, We order in increasing order of their degree.

What happens if the graph is not connected? We can pick a new starting node from unvisited nodes until all components are visited.

Let's work on an example. We have the following graph with its level sets:

![level sets](/images/school/spring-2024/cs475/6-2.png)

Then 
- $S_1 = \{s\}$
- $S_2 = \{a, b\}$
- $S_3 = \{e, c\}$
- $S_4 = \{d\}$

We start ordering from $S_1$ to $S_4$, within each level set, we order by degree. So the final ordering is $s, a, b, e, c, d$.

#### Reverse Cuthill-Mckee
This is just the ordering of Cuthill-McKee but in reverse. We can obtain this by performing Cuthill-Mckee, then reverse the ordering.

This has better performance than Cuthill-Mckee in terms of number of fill-ins. This is because the patterns produced by RCM are more like the low fill downward arrow matrix, rather than the upward arrow.

![CM](/images/school/spring-2024/cs475/6-3.png)

![comparison](/images/school/spring-2024/cs475/6-4.png)

<Theorem>
    <TheoremContent>
        If the graph is a tree, then the reverse Cuthill-McKee produces no fill-ins. (Not necessarily true for Cuthill-McKee)
    </TheoremContent>
</Theorem>
The intuition of this theorem is that the last ordering of CM is a leaf node with degree 1, so RCM puts the degree 1 node first, which results in less fill-ins.

## Matrix Re-ordering Part 2
Let's try to use the greedy strategy for reordering. A greedy strategy is an iterative strategy that makes the best choice at each step. For example, in the Knapsack problem, at each iteration, we pick whatever item that has the largest value. So, a greedy strategy for reordering would be to pick the node with least neighbours.

### Markowitz Reordering
Markowitz is a local rule that tries to approximately minimize fill-in on the current step only. In other words, it greedily minimizes fill-in for the current step.

When we perform $LU$ factorization, we subtract multiples of the current row $(k + 1)$ from rows below $(k + 2, \ldots, n)$, if there is a non-zero in the column to be zeroed out.

Specifically, fill-in does not occur in rows that already have a zero in the column, e.g.
$$
A^{(k)} = \begin{bmatrix}
    \textcolor{red}{\times} & \times & & \times \\
    \times & \textcolor{blue}{\times} & & \textcolor{blue}{\times} \\
    0 & & & \\
    \times & \textcolor{blue}{\times} & & \textcolor{blue}{\times}
\end{bmatrix}
$$
where the red $\times$ is the pivot element, and the blue $\times$ are the fill-ins.

The worst case fill-in at this step is 4 entries, which is 2 non-zeros in the row times the 2 non-zeros in the column.

We can swap rows and columns on the fly to reduce the reducing fill-in. The idea is to determine the entry that would mninimize the (worst-case) fill, then swap it into the top left position of $A^{(k)}$. The row that produces the least fill on this current step is determined using the Markowitz product.

Let $r_i^{(k)} = nnz$ (number of nonzeros) in row $i$ of $A^{(k)}$ and $c_j^{(k)} = nnz$ in column $j$ of $A^{(k)}$. The maximum possible fill using $a_{ij}^{(k)}$ as the pivot is
$$
(r_i^{(k)} - 1)(c_j^{(k)} - 1)
$$
which is called the **Markowitz product**.

The Markowitz reordering algorithm swaps rows/columns to choose the pivot $a_{pq}^{(k)}$ that minimizes the Markowitz product, that is
$$
(p, q) = \arg\min_{k \leq i, j} (r_i^{(k)} - 1)(c_j^{(k)} - 1)
$$
For example, if we have
$$
\begin{bmatrix}
    a & b & c & 0 \\
    d & e & 0 & 0 \\
    0 & f & g & 0 \\
    0 & h & 0 & i
\end{bmatrix}
$$
Then the Markowitz product for each entry is
$$
\begin{bmatrix}
    2 \times 1 & 2 \times 3 & 2 \times 1 & 0 \\
    1 \times 1 & 3 \times 1 & 0 & 0 \\
    0 & 3 \times 1 & 1 \times 1 & 0 \\
    0 & 3 \times 1 & 0 & 0 \times 0
\end{bmatrix}
$$
And we have that the $i$ entry has the smallest Markowitz product, so we choose $i$ as the pivot.

If $A$ is symmetric, then we select $a_{pp}^{(k)}$ with
$$
p = \arg\min_{k \leq i}(r_i^{(k)} - 1)
$$
since $\arg\min_{k \leq i} r_i^{(k)} = \arg\min_{k \leq j} c_j^{(k)}$. So we only need to consider the diagonal entries for symmetric matrices.

By symmetrically swapping both rows and columns, $a_{pp}^{(k)}$ becomes the new pivot. This approach has the following features:
- preserves symmetry and diagonal dominance
- corresponds to node reordering

<Definition>
    <DefinitionName>Diagonally Dominant</DefinitionName>
    <DefinitionContent>
        A matrix is weakly diagonally dominant if for every row, the magnitude of the diagonal entry is larger than or equal to the sum of the magnitudes of all the other entries in that row. That is,
        $$
        |a_{ii}| \geq \sum_{i \neq j} |a_{ij}| \quad \forall i
        $$
    </DefinitionContent>
</Definition>
### Minimum Degree Reordering
The symmetric case of Markowitz reordering inspires an algorithm called **minimum degree reordering**. Consider the $(r_i^{(k)} - 1)$ (the number of non-zero entries in the row) for diagonal entries of this symmetric matrix
$$
\begin{bmatrix}
    \times & \times & \times & \times & \times \\
    \times & \times & & & \\
    \times & & \times & \times & \\
    \times & & \times & \times & \times \\
    \times & & & \times & \times 
\end{bmatrix}
$$
We have
- $a_{11} \mapsto 4$
- $a_{22} \mapsto 1$
- $a_{33} \mapsto 2$
- $a_{44} \mapsto 3$
- $a_{55} \mapsto 2$

Notice that the value $(r_i^{(k)} - 1)$ is the degree of the node in the graph. Minimum degree ordering therefore chooses the node with (current) minimum degree as the pivot element, at each step of factorization. The corresponding graph at each step is called the **elimination graph**. Thus, we have the following minimum degree algorithm:
1. Construct the elimination graph
2. At each step,
    1. Number the node with current least degree
    2. Remove the node and its edges
    3. Add new edges connecting all its neighbours together
3. Relabel the graph accordingly

Let's work on an example. We break ties by alphabetical order. The final graph at the bottom is the resulting reordered graph.

![min degree](/images/school/spring-2024/cs475/7-1.png)

<Theorem>
    <TheoremContent>
        Minimum degree reordering is optimal for trees, i.e. it produces no fills on trees.
    </TheoremContent>
</Theorem>
So, minimum degree reordering tries to greedily minimize fill-ins at each step by eliminating the node with least degree. This often outperforms RCM.

### Image De-noising
Images often contain random noise (small errors), arising from sensors, capture methods, or lighting conditions.

Often there is enough signal amidst the noise that we can try to recover a version with the noise removed/reduced.

Image denoising is an **inverse problem**. That is, given some observations we want to reconstruct the source that generated them. So, given some noisy image $u^0$, of some signal $u^*$, we want to recover the clean signal $u^*$, i.e.
$$
u^0 = u^* + n
$$
where $n$ is the noise. So, we want to decompose the noisy image $u^0$ into the sum of two components: the clean signal $u^*$ and the noise $n$. It is often impossible to find $u^*$ exactly, but we can find an approximation of $u^*$.

We can treat grayscale images as 2D scalar functions, so $u_{ij}$ is the pixel intensity value at row $i$ and column $j$. There are two key assumptions enabling us to solve the inverse problem:
1. The noise is not too large, i.e. observation $u^0$ is close to signal $u^*$.
2. Signal $u^*$ has some **structure** that we can exploit.

We seek $u$ satisfying
$$
\min_u \alpha R(u) + \| u - u^0 \|_2^2
$$
where
- $\| u - u^0 \|_2^2$  is the measure of discrepancy between the observation $u^0$ and the numerical solution $u$.
- $R(u)$ is the **regularization model**. There are three models, Tikhonov, Laplacian, and Total Variation.
- $\alpha > 0$ is the **regularization constant**, which controls the trade-off between **regularity** (smoothness) and **fit** (closeness to $u^0$). As $\alpha \to 0$, we have $u \approx u^0$, so this outputs the observation $u^0$. As $\alpha \to \infty$, this means $u \approx \min R(u)$, which gives a perfectly regular image.

#### Tikhonov Regularization
The Tikhonov regularization model is
$$
R(u) = \| u \|^2_2
$$
so our optimization becomes
$$
\min_u \alpha \| u \|^2_2 + \| u - u^0 \|_2^2
$$
Solving this gives the solution
$$
u = \frac{u^0}{\alpha + 1}
$$
So, we can see that as $\alpha \to 0$, we get $u \approx u^0$. As $\alpha \to \infty$, we get $u \approx 0$.

#### Laplacian Regularization
Consider a noisy image in 1D. If we graph it out, we can see that there is drastic change in slope throughout the image, compared to a smoother 1D image where the slope chanages more continuously.

![1D image](/images/school/spring-2024/cs475/7-2.png)

Therefore, we should try to penalize changes in slopes/derivatives, i.e. $\nabla u$, instead of pixel values $u$. The Laplacian regularization model is
$$
R(u) = \| \nabla u \|^2_2
$$
So the optimization problem becomes
$$
\min_u \alpha \| \nabla u \|^2_2 + \| u - u^0 \|_2^2
$$
We can use the finite difference approximation of the Laplacian to get
$$
\frac{\alpha}{h^2} (4u_{ij} - u_{i-1, j} - u_{i+1, j} - u_{i, j-1} - u_{i, j+1}) + u_{ij} = u_{ij}^0
$$
This gives a matrix equation of the form $(\alpha A + I)u = u^0$. Then we can try to keep denoising it by iterating:
$$
(\alpha A + I) u^{k+1} = u^k, \quad k = 1,2,\ldots,K
$$
However, the drawback of Laplacian regularization is that it tends to smear out edges.

#### Total Variation Regularization
To avoid smearing edges, the total variation regularization is
$$
R(u) = \| \nabla u \|_1
$$
This still minimizes the slopes but with a different measure that does not punish them too much.

So our optimization becomes
$$
\min_u \alpha \| \nabla u \|_1 + \| u - u^0 \|_2^2
$$
This leads to another PDE to solve:
$$
-\alpha \nabla \cdot \left( \frac{\nabla u}{\| \nabla u \|_2} \right) + u = u^0
$$
The behaviour of total variation regularization is
1. Edge-like regions are smoothed less, and
2. flatter regions are smoothed more.

To solve systems of the form
$$
\alpha A(u) + u = u^0
$$
we can use fixed point iteration. That is, we solve
$$
\begin{align*}
    \alpha A(u^k) u^{k+1} + u^{k+1} &= u^0 \\
    \implies \alpha (A^{u^k} + I) u^{k+1} &= u^0 
\end{align*}
$$
for $k = 0,1,\ldots,K$. So we pick an initial guess and compute an approximate solution by solving the system. The matrix $A(u^k)$ is then recomputed for the next iteration.

Note that such an iteration does not always converge to the solution in general.

One approach to determine when to stop iterating is when the approximation is not changing anymore, i.e.
$$
\| u^{k+1} - u^k \| < \text{tolerance}
$$
for some small tolerance.

## Iterative Methods
We look at iterative methods for solving linear systems, these methods gradually and iteratively refine a solution. They repeat the same steps over and over, then stop only when a desired tolerance is achieved. Some possible benefits of iterative methods compared to direct methods:
1. They may be faster and may require less memory
2. In each iteration, we get an approximate solution. This enables one to control the error a bit.
3. Exact algorithms such as Gaussian elimination need to alter the matrix $A$. The splitting algorithms discussed below do not. If we cannot find a good ordering to significantly reduce the amount of fill-ins, then iterative algorithms should be used.

### Termination Criterion
Ideally, the termination criterion is based on the error $e = x - \hat x$ between
- the current approximate numerical solution $\hat x$ and
- the true solution $x$

We would like to terminate computation when $e \approx 0$, however we do not know the true solution $x$.

Instead, we use the norm of the residual
$$
r = b - A\hat x
$$
This makes sense because $r = 0$ if and only if $A\hat x = b$. Thus, the residual measures how much the current approximation fails to satisfy $Ax = b$. The residual and error satisfy $Ae = r$ because
$$
\begin{align*}
    Ax &= b \\
    \implies Ax - A\hat x &= b - A \hat x \\
    \implies A(x - \hat x) &= b - A \hat x \\
    \implies Ae &= r
\end{align*}
$$
The matrix condition number can be used to bound the relative size of error $e$ and residual $r$ as
$$
\frac{\| e \|}{\| x \|} \leq \kappa(A) \frac{\| r\|}{\|b\|}
$$
where the condition number $\kappa(A) = \|A\| \|A^{-1}\|$. Note that the condition number is greater than or equal to 1 since 
$$
\|A \| \|A^{-1} \| \geq \| I \| = 1
$$
From this, we see that a small residual can imply a small error, but only if $A$ is well conditioned ($\kappa$ is small).

### Stationary Iterative Methods
Iterative methods start from some arbitrary or zero guess at the solution. Increasingly accurate approximations to the solution are generated by iterating a basic procedure repeatedly.

We can rewrite the linear system $Ax = b$ as
$$
(M - N) x = b \iff Mx = Nx + b
$$
where $A = M - N$. We assume that $M$ is invertible, but we do not need to compute $M^{-1}$. Starting from some initial guess $x^0$, we can iteratively find $x$ by repeatedly solving
$$
Mx^{k + 1} = (Nx^k + b)
$$
Then we have
$$
\begin{align*}
    Mx^{k+1} &= Nx^k + b \\
    &= (M - A) x^k + b \\
    &= Mx^k - Ax^k + b \\
    &= Mx^k + (b - Ax^k)
\end{align*}
$$
Thus we get
$$ 
x^{k+1} = x^k + M^{-1} (b - Ax^k)
$$
For this to be effective, we need to choose $M$ and $N$ such that
1. It is easy to solve the linear system, i.e. $My = z$ should be easy to solve.
2. $M$ is close to $A$, in the sense of having small norm $\| I - M^{-1}A \|$.

We could choose $M = A$, and the iterative procedure will converge in one iteration since
$$
x^{k+1} = x^k + A^{-1}(b - Ax^k) = x^k + x - x^k = x
$$
but then we are finding $A^{-1}$, and in that case we might as well just solve $x = A^{-1}b$ directly. Finding the actual $A^{-1}$ is too expensive.

#### Richardson
Richardson is the simplest method. We choose $M$ to be
$$
M = \frac{1}{\theta} I
$$
where $\theta > 0$ is some appropriately chosen constant. So, the Richardson iteration is
$$
x^{k+1} = x^k + \theta(b - Ax^k)
$$
Each iteration costs $O(nnz(A))$, the number of nonzero entries of $A$.

#### Jacobi
The next three methods will rely on the following labelled submatrices of $A$:
1. $D$ is the diagonal matrix of $A$
2. $-L$ is the strictly lower triangular part of $A$
3. $-U$ is the strictly upper triangular part of $A$

Then the Jacobi iteration is $M = D$, so
$$
x^{k+1} = x^k + D^{-1}(b - Ax^k) 
$$
For each $i$th entry, it is
$$
\begin{align*}
    x^{k+1}_i &= x^k_i + \frac{1}{a_{ii}}\left(b_i - \sum_{j} a_{ij x_j^k}\right) \\
    &= x^k_i + \frac{1}{a_{ii}} \left(b_i - a_{ii} x_i^k - \sum_{j \neq i} a_{ij} x_j^k\right) \\
    &= \frac{1}{a_{ii}} \left(b_i - \sum_{j \neq i} a_{ij} x_j^k\right) 
\end{align*}
$$
Let's see the motivation for Jacobi. Recall the residual vector $r = b - Ax$. Clearly, $x$ is a solution iff $r = 0$. Given the current iterate $x^k$, the Jacobi iteration tries to zero out the $i$-th residual $r_i$,
$$
\begin{align*}
    r_i &= 0 \\
    \iff b_i - \sum_{j \neq i} a_{ij} x^k_j - a_{ii} x &= 0
\end{align*}
$$
Then rearranging to isolate $x$ gives
$$
x_i^{k+1} = \frac{1}{a_{ii}} \left(b_i - \sum_{j \neq i} a_{ij} x_j^k\right)
$$
The Jacobi iteration is easy to implement but quite slow, however it is trivially parallelizable.

#### Gauss-Seidel
The Gauss-Seidel iteration is very similar to the Jacobi iteration. The difference is instead of using old data $x^k$, we use the newest $x^{k+1}$ for entries that have already been updated so far on this pass.
$$
\begin{align*}
    x_i^{k+1} &= x_i^k + \frac{1}{a_{ii}} \left(b_i - \sum_{j < i} a_{ij} x_j^{k+1} - \sum_{j \geq i} a_{ij} x_j^k\right) \\
    &= \frac{1}{a_{ii}} \left(b_i - \sum_{j < i} a_{ij} x_j^{k+1} - \sum_{j > i} a_{ij} x_j^k\right)
\end{align*}
$$
In matrix form, this is
$$
x^{k+1} = x^k + (D - L)^{-1}(b - Ax^k)
$$
so $M = D - L$.

There are multiple variants of Gauss-Seidel. In the regular GS, we update $x^k$ from top to bottom. The reverse ordering gives backward Gauss Seidel, which is
$$x^{k+1} = x^k + (D - U)^{-1}(b - Ax^k)$$
where $M = D - U$. This updates $x_k$ in reverse order.

We can also combine forward GS and backward GS to construct symmetric GS.
$$
\begin{align*}
    x^{k + \frac{1}{2}} &= x^k + (D - L)^{-1}(b - Ax^k) \\
    x^{k+1} &= x^{k + \frac{1}{2}} + (D - U)^{-1}(b - Ax^{k + \frac{1}{2}})
    &= x^k + (D - U)^{-1} D(D - L)^{-1} (b - Ax^k)
\end{align*}
$$
where $x^{k+\frac{1}{2}}$ is the midpoint between $x^k$ and $x^{k+1}$. In matrix form, we have
$$
M = (D - L)D^{-1} (D - U)
$$
so
$$
x^{k+1} = x^k + (D - U)^{-1} D(D - L)^{-1} (b - Ax^k)
$$
- Gauss Seidel usually converges faster than the Jacobi iteration since we are using more up-to-date information.
- However, GS is an inherently sequential algorithm and is harder to parallelize.

#### Successive Overrelaxation (SOR)
A common strategy to accelerate fixed-point iterations is averaging. For instance, by averaging the current iterate $x^k$ with the GS update, we obtain SOR:
$$
\begin{align*}
    x_i^{k+1} &= (1 - \omega) x_i^k + \omega (x_i^{k+1})^{\text{GS}} \\
    &= (1 - \omega) x_i^k + \frac{\omega}{a_{ii}}\left(b_i - \sum_{j < i} a_{ij}x_j^{k+1} - \sum_{j > i} a_{ij}x_j^k\right)
\end{align*}
$$
or in matrix notation,
$$
x^{k+1} = (1 - \omega) x^k + \left(\frac{1}{\omega}D - L\right)^{-1}(b - Ax^k)
$$
hence for SOR the matrix $M$ is
$$
M = \frac{1}{\omega}D - L
$$
For $\omega = 1$, we get the GS iteration. For $0 < \omega < 1$, the update is called under-relaxation, while for $\omega > 1$ the update is called over-relaxation. For certain choices of $\omega > 1$, SOR may converge substantially faster than GS. The optimal $\omega$ normally lies in the range $1 < \omega < 2$.

#### Convergence
The key questions involving the convergence of splitting methods are:
1. under what conditions does the iteration converge to the correct solution?
2. if it does converge, how quickly does it do so?

These convergence questions depend on the **spectral radius** of $A$, denoted $\rho(A)$. The spectral radius is defined in terms of the eigenvalues of $A$.

<Definition>
    <DefinitionName>Spectral Radius</DefinitionName>
    <DefinitionContent>
        Recall that an aigenvalue $\lambda \in \mathbb R$ and corresponding eigenvector $v \in \mathbb R^n$ of $A \in \mathbb R^{n \times n}$ satisfy
        $$
        Av = \lambda v, \quad v \neq 0
        $$
        The spectral radius of $A$ is
        $$
        \rho(A) = \max_i \| \lambda_i \|
        $$
        where $\lambda_i$ are the eigenvalues of $A$. In other words, $\rho(A)$ is the largest in magnitude eigenvalue of $A$.
    </DefinitionContent>
</Definition>
We can rewrite our usual iteration as
$$
\begin{align*}
    x^{k+1} &= x^k + M^{-1}(b - Ax^k) \\
    &= (I - M^{-1}A)x^k + M^{-1}b
\end{align*}
$$
We call the matrix $I - M^{-1}A$ the **iteration matrix** for a particular method.

<Definition>
    <DefinitionName>Induced Matrix Norm</DefinitionName>
    <DefinitionContent>
        The induced matrix norm is defined as
        $$
        |\ A \| = \max_{\| x \| = 1} \| Ax \|
        $$
    </DefinitionContent>
</Definition>
We have the following theorem that gives a sufficient condition for convergence.
<Theorem withName={true}>
    <TheoremName>Convergence of Stationary Iterative Method</TheoremName>
    <TheoremContent>
        Let the true solution $x^*$ satisfy 
        $$
        x^* = (I - M^{-1}A)x^* + M^{-1}b
        $$
        If $\| I - M^{-1}A \| < 1$ for some induced matrix norm $\| \cdot \|$, then the stationary iterative method converges. That is, for any initial guess $x^0$,
        $$
        \lim_{k \to \infty} x^k = x^*
        $$
    </TheoremContent>
</Theorem>
<Proof>
    <ProofContent>
        We have $x^{k+1} = (I - M^{-1}A)x^k + M^{-1}b$ and exact solution $x^* = (I - M^{-1}A)x^* + M^{-1}b$. Subtracting the two gives
        $$
        \begin{align*}
            x^{*} - x^{k+1} &= (I - M^{-1}A)(x^* - x^k) \\
            \implies \| x^* - x^{k+1} \| &= \|(I - M^{-1}A)(x^* - x^k)\| \\
            &\leq \| I - M^{-1}A \| \| x^* - x^k \| \\
            \implies \|e^{k+1} \| \leq \| I - M^{-1}A \| \| e^k \|
        \end{align*}
        $$
        Note that the matrix norm above is the induced norm by the vector norm. Since $\| I - M^{-1}A \| < 1$, the magnitude of the error decreases at each iteration.
    </ProofContent>
</Proof>
<Theorem>
    <TheoremContent>
        The iterative method $x^{k+1} = x^k + M^{-1}(b - Ax^k)$ converges for any $x^0$ and $b$ if and only if $\rho(I - M^{-1}A) < 1$.
    </TheoremContent>
</Theorem>
- The speed of convergence depends on the size of $\rho(I - M^{-1}A)$ since the error satisfies
    $$
    \| x^{k+1} - x^* \| \leq \rho(I - M^{-1}A) \| x^k - x^* \|
    $$
- That is, the magnitude of the error scales by $\rho(I - M^{-1}A)$ on each iteration.
- We call $\rho(I - M^{-1}A)$ the **convergence factor**.
- Smaller $\rho$ implies faster convergence.