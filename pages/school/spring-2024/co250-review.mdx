import { Definition, DefinitionName, DefinitionContent, Corollary, CorollaryName, CorollaryContent, Example, ExampleName, ExampleContent, Problem, ProblemName, ProblemContent, Proposition, PropositionName, PropositionContent, Theorem, TheoremName, TheoremContent, Lemma, LemmaName, LemmaContent, Proof, ProofName, ProofContent } from "/components/math";
import { Steps, Callout } from 'nextra/components'

# CO 250 Review
This is a review of the material covered in CO 250. It is not meant to be comprehensive, but rather a summary of the key concepts.

## Linear Programs
First let's define some terms.
<Definition>
<DefinitionName>Feasible Solution</DefinitionName>
<DefinitionContent>
A solution is feasible if it has the property that all constraints are satisfied.
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Optimal Solution</DefinitionName>
<DefinitionContent>
An optimal solution to a formulation is a feasible solution that maximizes the objective function (or minimizes it, depending on the problem).
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Affine Function</DefinitionName>
<DefinitionContent>
A function $f$ is affine if it is of the form $f(x) = a^T x + b$, where $x$ and $a$ are vectors and $b$ is a real number. If $b = 0$, then $f$ is linear.

Note that every linear function is affine, but not every affine function is linear.
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Linear Constraint</DefinitionName>
<DefinitionContent>
A constraint is linear if it is of the following form:
- $f(x) \leq b$
- $f(x) \geq b$
- $f(x) = b$

where $f$ is a linear function and $b$ is a real number.
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Linear Program</DefinitionName>
<DefinitionContent>
A linear program (LP) is the problem of maximizing/minimizing an affine function subject to a finite number of linear constraints.
</DefinitionContent>
</Definition>

## Solving Linear Programs
<Definition>
<DefinitionName>Infeasible Linear Program</DefinitionName>
<DefinitionContent>
A linear program is infeasible if there does not exist a solution that satisfies all constraints simultaneously.
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Unbounded Linear Program</DefinitionName>
<DefinitionContent>
A linear program is unbounded if the objective function can be made arbitrarily large (in the case of maximization) or arbitrarily small (in the case of minimization) without violating any constraints. That is, given objective function $f(x)$, for every $\alpha \in \mathbb R$, there exists a feasible solution $x$ such that $f(x) \geq \alpha$ (for maximization problems) or $f(x) \leq \alpha$ (for minimization problems).
</DefinitionContent>
</Definition>
<Theorem withName={true}>
<TheoremName>Fundamental Theorem of Linear Programming</TheoremName>
<TheoremContent>
Every linear program is exactly one of the following:
- has an optimal solution,
- is infeasible
- is unbounded.
</TheoremContent>
</Theorem>
<Definition>
<DefinitionName>Standard Equality Form (SEF)</DefinitionName>
<DefinitionContent>
An LP is in SEF if
- it is a maximization problem
- all constraints are equality constraints except for variable non-negativity constraints (i.e. $x_i \geq 0$ for all $i$).
- all variables are non-negative.

For example, the following is in SEF:
$$
\begin{align*}
\text{max } & 2x_1 + 3x_2 \\
\text{s.t } & x_1 + x_2 = 4 \\
& x_1, x_2 \geq 0
\end{align*}
$$
</DefinitionContent>
</Definition>

### Transforming LP to SEF
When we are given a minimization problem, we can multiply the objective function by $-1$ to get a maximization problem. For example, if we have the minimization problem
$$
\min{2x_1 - 3x_2 + 7}
$$
we can transform it into the maximization problem
$$
\max{-2x_1 + 3x_2 - 7}
$$
For $\geq$ constraints, we can subtract a slack variable to transform it into an equality constraint. For $\leq$ constraints, we can add a slack variable to transform it into an equality constraint.

Then for each variable $x_i$ that does not have a non-negativity constraint, we can write it as the difference of two non-negative variables:
$$
x_i = x_i^+ - x_i^-
$$
and set $x_i^+, x_i^- \geq 0$.

<Example withName={true}>
<ExampleName>Transforming to SEF</ExampleName>
<ExampleContent>
Suppose we are given
$$
\begin{align*}
\text{min } &2x_1 - 3x_2 + 7 \\
\text{s.t } &x_1 + x_2 \geq 3 \\
&-3x_1 + x_2 \leq 5 \\
& x_2 \geq 0
\end{align*}
$$
We can transform it into SEF as follows:
$$
\begin{align*}
\text{max } &-2(x_1^+ - x_1^-) + 3x_2 - 7 \\
\text{s.t } &x_1^+ - x_1^- + x_2 - x_3 = 3 \\
&-3(x_1^+ - x_1^-) + x_2 + x_4 = 5 \\
& x_1^+, x_1^-, x_2, x_3, x_4 \geq 0
\end{align*}
$$
</ExampleContent>
</Example>

### Basis and Basic Solutions
<Definition>
<DefinitionName>Column Sub-matrix</DefinitionName>
<DefinitionContent>
Suppose that $B$ is a subset of the column indices of the matrix $A$. Then $A_B$ is a column sub-matrix of $A$ that consists of the columns of $A$ indexed by $B$.

For example, if 
$$
A = \begin{bmatrix}
1 & 2 & 3 & 4 & 5 \\
6 & 7 & 8 & 9 & 10 \\
11 & 12 & 13 & 14 & 15
\end{bmatrix}, \quad B = \{1, 3, 5\}
$$
then
$$
A_B = \begin{bmatrix}
1 & 3 & 5 \\
6 & 8 & 10 \\
11 & 13 & 15
\end{bmatrix}
$$
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Basis</DefinitionName>
<DefinitionContent>
Let $B$ be a subset of column indices of $A$. Then $B$ is a basis if
1. $A_B$ is a square matrix
2. $A_B$ is invertible (i.e. columns are independent)
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Basic Variables</DefinitionName>
<DefinitionContent>
Let $B$ be a basis for $A$
- if $j \in B$ then $x_j$ is a basic variable
- if $j \not \in B$ then $x_j$ is a non-basic variable
</DefinitionContent>
</Definition>
<Definition>
<DefinitionName>Basic Solution</DefinitionName>
<DefinitionContent>
For an LP with system $Ax = b$, $\bar x$ is a basic solution for basis $B$ if
- $A\bar x = b$
- $x_j = 0$ for all $j \not \in B$

For example, if we have
$$
A = \begin{bmatrix}
1 & 2 & -1 & 1 & -1 \\
0 & 1 & 0 & 1 & -1 \\
0 & 0 & 1 & 1 & -1 
\end{bmatrix}, \quad b = \begin{bmatrix}
2 \\ 1 \\ 1
\end{bmatrix}
$$
Then $x = \begin{bmatrix} 1 & 1 & 1 & 0 & 0 \end{bmatrix}^T$ is a basic solution for basis $B = \{1, 2, 3\}$.
</DefinitionContent>
</Definition>
<Theorem>
<TheoremContent>
Consider $Ax = b$ and a basis $B$ of $A$. Then there exists a unique basic solution $\bar x$ for basis $B$.
</TheoremContent>
</Theorem>
<Definition>
<DefinitionName>Canonical Form</DefinitionName>
<DefinitionContent>
Consider the problem $P$:
$$
\max\{c^T x: Ax = b, x \geq 0\}
$$
Let $B$ be a basis of $A$. Then the problem $P$ is in canonical form for $B$ if
1. $A_B = I$, and
2. $c_j = 0$ for all $j \in B$ (the coefficient of $x_j$ in the objective function is 0)
</DefinitionContent>
</Definition>
And now we have the Simplex Algorithm which solves an LP.
<Definition>
<DefinitionName>Simplex Algorithm</DefinitionName>
<DefinitionContent>
Suppose we have a basic solution to the LP problem in canonical form with basis $B$. We can find a better feasible solution by
1. Pick $k \not \in B$ such that $c_k > 0$
2. Set $x_k = t \geq 0$ as large as possible
3. Keep all other non-basic variables at $0$
4. Choose basic variables such that $Ax = b$ holds.
</DefinitionContent>
</Definition>

<Definition>
<DefinitionName>Bland's Rule</DefinitionName>
<DefinitionContent>
- If we have a choice for the element entering the basis, pick the one with the smallest index.
- If we have a choice for the element leaving the basis, pick the one with the smallest index.
</DefinitionContent>
</Definition>
Then, we will demonstrate the tableau method for Simplex Algorithm.

<Example withName={true}>
<ExampleName>Simplex Algorithm with Tableau</ExampleName>
<ExampleContent>
Suppose that we have the following LP in SEF and initial basis $B = \{3, 4\}$:
$$
\begin{align*}
\text{max } & \begin{pmatrix}2 & 3 & 0 & 0\end{pmatrix}x \\
\text{s.t } & \begin{pmatrix}
1 & 2 & 1 & 0 \\
5 & 5 & 0 & 1
\end{pmatrix}x = \begin{pmatrix} 20 \\ 60 \end{pmatrix} \\
&x \geq 0
\end{align*}
$$
Then we have the initial Tableau

$$
\begin{array}{c|cccc|c}
1 & -2 & -3 & 0 & 0 & 0 \\
\hline
0 & 1 & 2 & 1 & 0 & 20 \\
0 & 5 & 5 & 0 & 1 & 60\\
\end{array}, \quad B = \{3, 4\}
$$
Since there are two choices for the variable entering the basis, we choose the smaller index, so $x_1$ enters the basis. Then we do the ratio test. Since $\frac{60}{5} = 12 < 20 = \frac{20}{1}$, the leaving variable is $x_4$. So we perform row operations so that we get a canonical form with respect to basis $B = \{1, 3\}$:
$$
\begin{array}{c|cccc|c}
1 & 0 & -1 & 0 & \frac{2}{5} & 24 \\
\hline
0 & 1 & 1 & 0 & \frac{1}{5} & 12 \\
0 & 0 & 1 & 1 & -\frac{1}{5} & 8\\
\end{array}, \quad B = \{1, 3\}
$$
Now, we can only choose $x_2$ as the entering variable. Since $\frac{8}{1} = 8 < 12 = \frac{12}{1}$ we choose $x_3$ to be the leaving variable. So we perform row operations so that we get a canonical form with respect to basis $B = \{1, 2\}$:
$$
\begin{array}{c|cccc|c}
1 & 0 & 0 & 1 & \frac{1}{5} & 32 \\
\hline
0 & 1 & 0 & -1 & \frac{2}{5} & 4 \\
0 & 0 & 1 & 1 & -\frac{1}{5} & 8\\
\end{array}, \quad B = \{1, 2\}
$$
Since there are no more negative coefficients in the first row, we stop. So $B = \{1,2\}$ is the optimal basis. We read off the right hand side to get the optimal solution $x = \begin{pmatrix} 4 & 8 & 0 & 0 \end{pmatrix}^T$.

This is equivalent to the LP
$$
\begin{align*}
\text{max } & \begin{pmatrix} 0 & 0 & -1 & -\frac{1}{5} \end{pmatrix}x + 32 \\
\text{s.t } & \begin{pmatrix}
1 & 0 & -1 & \frac{2}{5} \\
0 & 1 & 1 & -\frac{1}{5}
\end{pmatrix}x = \begin{pmatrix} 4 \\ 8 \end{pmatrix} \\
&x \geq 0
\end{align*}
$$
Since our optimal solution is $x = \begin{pmatrix} 4 & 8 & 0 & 0 \end{pmatrix}^T$, the optimal value is $32$.
</ExampleContent>
</Example>

## Duality
<Definition>
<DefinitionName>Primal-dual Pair</DefinitionName>
<DefinitionContent>
Given a LP $(P)$ in SEF 
$$
\max\{c^Tx: Ax = b, x \geq 0\}
$$
its dual LP $(D)$ is
$$
\min\{b^Ty: A^Ty \geq c\}
$$
Note that there is a correspondence between constraints and dual variables.
- A constraint (row) in the primal corresponds to a variable in the dual (column).
- A variable in the primal (column) corresponds to a constraint in the dual (row).

and vice versa
</DefinitionContent>
</Definition>
<Theorem withName={true}>
<TheoremName>Weak Duality Theorem</TheoremName>
<TheoremContent>
Any feasible solution to the dual gives an upper bound on the objective values of **all** feasible primal solutions. In particular, the objective values of dual feasible solutions must be greater than or equal to the objective values of primal feasible solutions.

Therefore, if there exists a dual and a primal feasible solution with the same objective value, then they must be optimal solutions.
</TheoremContent>
</Theorem>

<Theorem withName={true}>
<TheoremName>Strong Duality Theorem</TheoremName>
<TheoremContent>
Let $(P)$ and $(D)$ be the primal and dual LPs. If $(P)$ has an optimal solution, then $(D)$ has an optimal solution and the optimal values are equal.
</TheoremContent>
</Theorem>
So, we have the following outcomes of primal-dual pairs.
| Dual $\downarrow$ Primal $\rightarrow$ | Optimal | Unbounded | Infeasible |
|---------------------------------------|---------|-----------|------------|
| Optimal | Possible (S) | Impossible (S) | Impossible (S) |
| Unbounded | Impossible (S & W) | Impossible (W) | Possible (W) |
| Infeasible | Impossible (S) | Possible (W) | Possible |

### Finding the Dual for any LP
Here is a table that summarizes how to convert different types of variables and constraints:
| max LP | min LP |
|--------|--------|
| $\leq$ constraints | $\geq 0$ variables |
| $=$ constraints | free variables |
| $\geq$ constraints | $\leq 0$ variables |
| $\geq 0$ variables | $\geq$ constraints |
| $\leq 0$ variables | $\leq$ constraints |
| free variables | $=$ constraints |

<Example>
<ExampleContent>
Suppose that we have $(P)$:
$$
\begin{align*}
\text{max } & \begin{pmatrix} 3 & 1 & 4 & 1 \end{pmatrix}x \\
\text{s.t } & \begin{pmatrix}
1 & 2 & 3 & 4 \\
4 & 3 & 2 & 1 \\
2 & 2 & 2 & 2
\end{pmatrix}x \begin{array}{c} \geq \\ \leq \\ = \end{array} \begin{pmatrix} 3 \\ 4 \\ 5 \end{pmatrix} \\
&x_1, x_2 \geq 0, x_3 \leq 0, x_4 \text{ free}
\end{align*}
$$
Then its dual $(D)$ is
$$
\begin{align*}
\text{min } & \begin{pmatrix} 3 & 4 & 5 \end{pmatrix}y \\
\text{s.t } & \begin{pmatrix}
1 & 4 & 2 \\
2 & 3 & 2 \\
3 & 2 & 2 \\
4 & 1 & 2
\end{pmatrix}y \begin{array}{c} \geq \\ \geq \\ \leq \\ = \end{array} \begin{pmatrix} 3 \\ 1 \\ 4 \\ 1 \end{pmatrix} \\
&y_1 \leq 0, y_2 \geq 0, y_3 \text{ free}
\end{align*}
$$
</ExampleContent>
</Example>

### Complementary Slackness
The complementary slackness conditions are as follows. Given any primal-dual pair $(P),(D)$:
- $x_i = 0$ or the corresponding dual constraint is tight, and
- $y_j = 0$ or the corresponding primal constraint is tight.

And we have a theorem:
<Theorem withName={true}>
<TheoremName>Complementary Slackness Theorem</TheoremName>
<TheoremContent>
Let $(P), (D)$ be a primal-dual pair. Let $\bar x, \bar y$ be feasible solutions for $(P), (D)$ respectively. Then $\bar x$ and $\bar y$ are optimal for $(P), (D)$ if and only if $\bar x, \bar y$ satisfy all complementary slackness conditions.
</TheoremContent>
</Theorem>

<Example withName={true}>
<ExampleName>Using Complementary Slackness to Certify Optimality</ExampleName>
<ExampleContent>
Suppose we have $(P)$:
$$
\begin{align*}
\text{max } & \begin{pmatrix} 1 & -2 \end{pmatrix}x \\
\text{s.t } & \begin{pmatrix}
1 & -1 \\
2 & 1 \\
0 & 1
\end{pmatrix}x \begin{array}{c} = \\ \geq \\ \leq \end{array} \begin{pmatrix} 2 \\ 1 \\ 4 \end{pmatrix} \\
&x_1\geq 0, x_2 \text{ free}
\end{align*}
$$
Then its dual $(D)$ is
$$
\begin{align*}
\text{min } & \begin{pmatrix} 2 & 1 & 4 \end{pmatrix}y \\
\text{s.t } & \begin{pmatrix}
1 & 2 & 0 \\
-1 & 1 & 1
\end{pmatrix}y \begin{array}{c} \geq \\ = \end{array} \begin{pmatrix} 1 \\ -2 \end{pmatrix} \\
&y_1 \text{ free}, y_2 \leq 0, y_3 \geq 0
\end{align*}
$$
Suppose we think that $\bar x = (1, -1)^T$ is optimal for $(P)$. We want to derive an optimal $\bar y$ for $(D)$. So we can check our CS conditions:
1. $ x_1 = 0$ or $y_1 + 2y_2 + 0y_3 = 1$.
    
    Since we have $\bar x_1 = 1 \neq 0$, we must have $y_1 + 2y_2 + 0y_3 = 1$.
2. $ x_2 = 0$ or $-y_1 + y_2 + y_3 = -2$

    In the dual, this constraint is an equality constraint, so we must have $-y_1 + y_2 + y_3 = -2$.
3. $y_1 = 0$ or $x_1 - x_2 = 2$

    In the primal, this constraint is an equality constraint, so this does not help us deduce $y_1$.
4. $y_2 = 0$ or $2x_1 + x_2 = 1$

    Since $2\bar x_1 + \bar x_2 = 1$, we cannot deduce anything about $y_2$.
5. $y_3 = 0$ or $0x_1 + x_2 = 4$

    Since $\bar x_2 \neq 4$, we must have $y_3 = 0$.

So we have the following system of equations:
$$
\begin{align}
y_1 + 2y_2 &= 1 \\
-y_1 + y_2 + y_3 &= -2 \\
y_3 &= 0
\end{align}
$$
Adding (2) to (1), and taking into account (3), we get
$$
3y_2 = -1 \implies y_2 = - \frac{1}{3}
$$
Substituting this into (1), we get
$$
y_1 - \frac{2}{3} = 1 \implies y_1 = \frac{5}{3}
$$
So we have optimal solution $\bar y = \begin{pmatrix} \frac{5}{3} & -\frac{1}{3} & 0 \end{pmatrix}^T$.
</ExampleContent>
</Example>

